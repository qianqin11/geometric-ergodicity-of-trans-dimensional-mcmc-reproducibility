\documentclass[12pt]{article}
\usepackage{amsbsy,amsmath,amsthm,amssymb,subcaption}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{hyperref}
\usepackage{bm}
\usepackage{xr}
\externaldocument{RJMCMC-Supplement}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

\makeatletter
\def\namedlabel#1#2{\begingroup
	#2%
	\def\@currentlabel{#2}%
	\phantomsection\label{#1}\endgroup
}
\makeatother

\newcommand{\df}{\mathrm{d}}
\newcommand{\X}{\mathsf{X}}
\newcommand{\Y}{\mathsf{Y}}
\newcommand{\Z}{\mathsf{Z}}
\newcommand{\SF}{\mathcal{A}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Par}{\X}
\newcommand{\Q}{\hat{Q}}
\newcommand{\prior}{m^{\scriptsize\mbox{pr}}}
\newcommand{\Prior}{M^{\scriptsize\mbox{pr}}}
\newcommand{\pprior}{p^{\scriptsize\mbox{pr}}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\Mtk}{\mtkfont{T}}
\newcommand{\mtkfont}{\mathcal}
\newcommand{\pcite}[1]{\citeauthor{#1}'s \citeyearpar{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\usepackage{xcolor}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%




\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Geometric ergodicity of trans-dimensional Markov chain Monte Carlo algorithms}
  \author{}
  \maketitle
} \fi

\if0\blind
{
  \title{\bf Geometric ergodicity of trans-dimensional Markov chain Monte Carlo algorithms}
  \author{Qian Qin\thanks{
  		Partially supported by NSF DMS-2112887}\hspace{.2cm}\\
  	School of Statistics, University of Minnesota}
  \maketitle
} \fi

\bigskip
\begin{abstract}
This article studies the convergence properties of trans-dimensional MCMC algorithms when the total number of models is finite.
It is shown that, for reversible and some non-reversible trans-dimensional Markov chains, under mild conditions, geometric convergence is guaranteed if the Markov chains associated with the within-model moves are geometrically ergodic.
This result is proved in an $L^2$ framework using the technique of Markov chain decomposition.
While the technique was previously developed for reversible chains, this work extends it to the point that it can be applied to some commonly used non-reversible chains.
The theory herein is applied to reversible jump algorithms for three Bayesian models: a probit regression with variable selection, a Gaussian mixture model with unknown number of components, and an autoregression with Laplace errors and unknown model order.
\end{abstract}

\noindent%
{\it Keywords:}  convergence rate, Markov chain decomposition, reversible jump, spectral gap.
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!

	\section{Introduction} \label{sec:intro}

In many statistical setups, the parameter space of interest is a union of disjoint subsets, where each subset corresponds to a model, and the dimensions of the subsets need not be the same.
Trans-dimensional Markov chain Monte Carlo (MCMC) is a class of algorithms for sampling from distributions defined on such spaces, which allows for model selection as well as parameter estimation.
%	Unlike traditional MCMC methods that operate within fixed-dimensional spaces, trans-dimensional MCMC allows for model selection and estimation by exploring multiple regions of the space in a single simulation.
This type of algorithm, especially the reversible jump MCMC developed by \cite{green1995reversible}, has been applied to important problems like change-point estimation \citep{green1995reversible}, autoregression models \citep{troughton1998reversible, ehlers2002efficient, vermaak2004reversible}, variable selection \citep{chevallier2022reversible}, wavelet models \citep{cornish2015bayeswave} etc.
The current article aims to provide conditions on geometric ergodicity for trans-dimensional Markov chains when the total number of models is finite.

Let $\mathsf{K}$ be a finite set whose elements are referred to as ``models."
A model~$k$ in~$\mathsf{K}$ is associated with a non-empty measurable space $(\Z_k, \A_k)$ and a nonzero finite measure $\Psi_k$ on $(\Z_k, \A_k)$.
Let $\X = \bigcup_{k \in \mathsf{K}} \{k\} \times \Z_k$, and let $\SF$ be the sigma algebra generated by sets of the form $\{k\} \times \mathsf{A}$, where $k \in \mathsf{K}$ and $\mathsf{A} \in \A_k$.
Consider the task of sampling from the probability measure~$\Pi$ on $(\X, \SF)$ such that
\begin{equation} \label{eq:Pi}
	\Pi(\{k\} \times \mathsf{A}) = \frac{\Psi_k(\mathsf{A})}{ \sum_{k' \in \mathsf{K}} \Psi_{k'}(\Z_{k'}) }, \quad k \in \mathsf{K}, \; \mathsf{A} \in \A_k.
\end{equation}
Suppose that a procedure generates a random element $(K,Z) \sim \Pi$.
Then, for $k \in \mathsf{K}$, $\Psi_k(\Z_k)/ \sum_{k' \in \mathsf{K}} \Psi_{k'}(\Z_{k'})$ gives the probability of $K = k$, and $\Phi_k(\cdot) = \Psi_k(\cdot)/\Psi_k(\Z_k)$ gives the conditional distribution of~$Z$ given $K = k$.



In practice,~$\Pi$ is often intractable, prompting the use of trans-dimensional MCMC methods.
A central goal of the current work is to provide verifiable sufficient conditions for trans-dimensional MCMC algorithms to be geometrically convergent in the $L^2$ distance.
When~$\SF$ is countably generated and the Markov chain is $\varphi$-irreducible, $L^2$ geometric convergence implies the classical notion of $\Pi$-a.e. geometric ergdoicity.
See \cite{roberts1997geometric}, \cite{roberts2001geometric}, and \cite{gallegos2023equivalences}.
Geometric ergodicity is one of the key conditions ensuring the reliability of MCMC estimation.
It guarantees a central limit theorem (CLT) for ergodic sums \citep{jones2001honest}; moreover, consistent uncertainty assessment through asymptotically valid confidence intervals is possible under geometric ergodicity \citep{vats2019multivariate}.


%	This work focuses on the case where the total number of models is finite.
%	Many model selection problems fall into this category \citep[see, e.g.,][]{green1995reversible,troughton1998reversible,chevallier2022reversible}.
%	However, the framework herein does not cover problems like spatial point processes, which have a large literature on their own.
%	See \cite{geyer2019likelihood} and references therein.


The convergence behavior of trans-dimensional MCMC algorithms is in general far from well understood.
\cite{roberts2006harris} established some general conditions for trans-dimensional Markov chains to be Harris recurrent.
Geometric ergodicity of some specific trans-dimensional algorithms was established in \cite{geyer1994simulation}, \cite{andrieu1999joint}, \cite{ortner2006reversible}, and \cite{schreck2015shrinkage}.
Existing proofs of geometric ergodicity often rely on drift and minorization conditions, or in some simple situations, Doeblin's condition.
The current work instead utilizes the decomposition of Markov chains, a remarkable technique developed by \cite{caracciolo1992two} and documented in \cite{madras2002markov}.
This technique allows one to decompose the dynamic of a trans-dimensional Markov chain into within- and between-model movements, which can be analyzed separately.
Using an extended version of this technique and exploiting the assumption that $|\mathsf{K}| < \infty$, Theorem~\ref{thm:main} is established.
This result describes a divide-and-conquer paradigm that enables one to establish geometric convergence of the trans-dimensional chain by combining the geometric ergodicity of its within-model components.
Quantitative bounds on the convergence rate will also be provided; see Theorem~\ref{thm:quantitative}.


Previously, Markov chain decomposition has found its use in important problems like simulated and parallel tempering.
See, e.g., \cite{madras2002markov, woodard2009conditions, ge2018simulated}.
This technique can be used to analyze a Markov chain whose state space can be partitioned into subsets such that, within each subset, the Markov chain's behavior is easy to analyze.
It was originally developed for reversible Markov chains.
The current work provides an extended version of the technique, given in Lemma~\ref{lem:decomposition}, that can deal with some important non-reversible chains.
(Despite their name, reversible jump algorithms can often be non-reversible.)


%	While $L^2$ geometric convergence guarantees geometric ergodicity in an almost sure sense, it does not ensure Harris ergodicity, i.e., Harris recurrence in addition to aperiodicity and $\varphi$-irreducibility.
%	Many theoretical results for Markov chain samples, such as the law of large numbers and the central limit theorem, are typically stated when assuming Harris ergodicity.
%	\cite{roberts2006harris} provided some useful conditions that lead to Harris recurrence in trans-dimensional settings.
%	However, these conditions could be tricky to verify in practice.
%	In Lemma~\ref{lem:ae}, it is shown that $\Pi$-a.e. geometric convergence is, in a broad sense, as good as Harris ergodicity, so long as the chain's starting distribution is absolutely continuous with respect to~$\Pi$.
%	
%	This article also discusses how geometric convergence allows for uncertainty quantification in Monte Carlo estimation in trans-dimensional settings.
%	In particular, it studies the construction of simultaneous confidence intervals based on a Monte Carlo sample.
%	The study is based on a method developed by \cite{robertson2020assessing} along with works on Markov chain CLT and asymptotic variance estimation.
%	A key challenge in trans-dimensional settings is that the asymptotic covariance matrix of a vector-valued Monte Carlo estimator is typically singular.
%	This makes existing methods difficult to apply or justify.
%	To circumvent the problem, a small amount of random noise is injected into the Monte Carlo estimator to turn the asymptotic covariance matrix non-singular.
%	This results in simultaneous confidence intervals that are theoretically correct and practically stable.



The theory developed herein is applied to reversible jump MCMC algorithms for three practical Bayesian models:
a probit regression model with variable selection, a Gaussian mixture model with unknown number of components, and an autoregressive model with Laplace errors and unknown model order.
Among these algorithms, only one is assuredly reversible. 
For each algorithm, we demonstrate geometric convergence. 
Enabled by geometric ergodicity, we also conduct Monte Carlo uncertainty assessments.

%This work focuses on the case where the number of models is finite.
%Many model selection problems fall into this category \citep[see, e.g.,][]{green1995reversible,troughton1998reversible,chevallier2022reversible}, although problems like spatial point processes do not.

Finally, it must be emphasized that verifying geometric ergodicity is but one of the first steps towards fully understanding the convergence behavior of an MCMC algorithm.
A chain being geometrically convergent does not ensure that it has a fast convergence rate.
While this work does provide a quantitative convergence rate bound, calculating the quantities involved in the bound can be practically challenging.
Obtaining sharp estimates for the convergence rate remains an open problem for most practical trans-dimensional MCMC algorithms.


The rest of this article is organized as follows.
Following a quick overview of the main qualitative result of this article, Section~\ref{sec:l2} contains some preliminary facts on the $L^2$ theory of Markov chains.
The main technical results involving Markov chain decomposition and the convergence rate of trans-dimensional MCMC are given in Section~\ref{sec:main}.
%	Section~\ref{sec:ergodicity} contains a discussion on the implications of almost everywhere ergodicity.
%	Section~\ref{sec:uncertain} studies the problem of uncertainty quantification in trans-dimensional Monte Carlo simulation.
One toy and two practical examples are studied in Section~\ref{sec:examples}, followed by a brief discussion in Section \ref{sec:discussion}.
\ref{s1} contains some minor results and technical proofs.
\ref{s2} contains yet another practical example.


\subsection{Conditions for geometric ergodicity: An overview}


Consider a trans-dimensional Markov chain $(X(t))_{t=0}^{\infty} = (K(t), Z(t))_{t=0}^{\infty}$ whose state space is~$\X$.
Let $P: \X \times \SF \to [0,1]$ be its Markov transition kernel (Mtk), i.e., for $(k,z) \in \X$ and $\mathsf{A} \in \SF$, $P((k,z), \mathsf{A})$ is understood as the conditional probability of $(K(t+1), Z(t+1)) \in \mathsf{A}$ given $(K(t), Z(t)) = (k,z)$.
Suppose that~$\Pi$ is a stationary distribution of this chain, i.e., $\Pi = \Pi P$, or more explicitly, for $\bigcup_{k \in \mathsf{K}} \{k\} \times \mathsf{A}_k \in \SF$,
\[
\sum_{k' \in \mathsf{K}} \Psi_{k'}(\mathsf{A}_{k'}) = \sum_{k \in \mathsf{K}} \sum_{k' \in \mathsf{K}} \int_{\Z_k} \Psi_k(\df z) P((k,z),  \{k'\} \times \mathsf{A}_{k'}).
\]
%	Let us decompose the chain's dynamic into within- and between-model movements.


%	Given $k \in \mathsf{K}$, $(z, A) \mapsto P((k,z), \{k\} \times A)$ describes how the chain moves within the model, i.e., inside $\{k\} \times \Z_k$.
%	To outline the between-model jumps, consider, for $(k,k') \in \mathsf{K} \times \mathsf{K}$, whether a move from model~$k$ to model~$k'$ is possible.
%	To be specific, let $\gamma_{k,k'} = 1$ if
%	\[
%	\int_{\Z_k} \Psi_k(\df z) P((k,z), \{k'\} \times \Z_{k'} ) > 0,
%	\]
%	and $\gamma_{k,k'} = 0$ otherwise.
%	List the elements of $\mathsf{K}$ in an arbitrary order, say, $k_{(1)}, k_{(2)}, \dots, k_{(|\mathsf{K}|)}$.
%	Let $\Gamma_P$ be the $|\mathsf{K}| \times |\mathsf{K}|$ matrix whose $i,j$th element is $\gamma_{k_{(i)}, k_{(j)}}$ for $i,j \in \{1,\dots,|\mathsf{K}|\}$.



The main results of this paper are stated in terms the $L^2$ theory for Markov chains, which is reviewed in Section~\ref{sec:l2}.
Essentially, if $\Mtk(\cdot,\cdot)$ is an Mtk that has a stationary distribution~$\omega$, then~$\Mtk$ can be regarded as a bounded linear operator on a certain Hilbert space $L_0^2(\omega)$.
A sufficient condition for the corresponding chain to be $L^2$ geometrically convergent is that the operator norm of some power of~$\Mtk$ is less than one.

One of the main results of this paper is stated below.
See Section~\ref{sec:main} for more details.

\begin{theorem} \label{thm:main}
	Assume that each of the following conditions holds for the trans-dimensional chain:
	\begin{enumerate} 
		\item [\namedlabel{H1}{(H1)}] There are a positive integer~$t_0$ and a sequence of Mtks $P_k: \Z_k \times \A_k \to [0,1], \; k \in \mathsf{K},$ such that the following properties hold for each~$k$:
		\begin{enumerate}
			\item [(i)] $\Phi_k P_k = \Phi_k$, where $\Phi_k(\cdot) = \Psi_k(\cdot)/\Psi_k(\Z_k)$ is the normalization of $\Psi_k(\cdot)$.
			\item [(ii)] When $P_k$ is regarded as an operator on $L_0^2(\Phi_k)$, the norm of its $t_0$'th power $P_k^{t_0}$ is strictly less than~1.
			\item [(iii)] there exists a constant $c_k > 0$ such that 
			$
			P((k,z), \{k\} \times \mathsf{A}) \geq c_k P_k(z, \mathsf{A})
			$ for $z \in \Z_k$ and $\mathsf{A} \in \A_k$.
		\end{enumerate} 
		
		\item [\namedlabel{H2}{(H2)}] 
		{ The between-model movements are irreducible.
		To be precise, the Mtk $\bar{P}: \mathsf{K} \times 2^{\mathsf{K}} \to [0,1]$ given by $
			\bar{P}(k,\{k'\}) = \int_{\Z_k} \Phi_k(\df z) P((k,z) , \{k'\} \times \Z_{k'}), \; k, k' \in \mathsf{K},
			$
		is irreducible.}
		%			The between-model movements are irreducible in the sense that, for $k, k' \in \mathsf{K}$, there exists a positive integer $t(k,k')$ such that
		%			\[
		%			\int_{\Z_k} \Psi_k(\df z) P^{t(k,k')}((k,z), \{k'\} \times \Z_{k'}) > 0.
		%			\]
	\end{enumerate}
	Then the norm of $P^{t_0}$ is strictly less than one, and the trans-dimensional chain is $L^2(\Pi)$ geometrically convergent.
\end{theorem}


Note that $\bar{P}(k,\{k'\})$ can be understood as the average probability flow from model~$k$ to model~$k'$, and thus $\bar{P}$ characterizes the between movements.
{ Evidently, \ref{H2} holds as long as the chain is $\Pi$-irreducible.
We give a rigorous proof of this assertion in Section \ref{app:h2} of \ref{s1}.}
Hence, in practice, \ref{H2} usually trivially holds.




Trans-dimensional MCMC algorithms typically involve a within-model move type, where the underlying chain stays in a model, say~$k$, with probability~$c_k$, and move according to an Mtk $P_k$ such that $\Phi_k P_k = \Phi_k$.
Then such $c_k$ and $P_k$ satisfy (i) and (iii) in \ref{H1}.
Condition (ii) in \ref{H1} requires a careful analysis of the within-model moves of an algorithm.
According to Lemma~\ref{lem:Pk-convergence} below, in several important situations, this condition is implied by the geometric ergodicity of the chain associated with $P_k$, or some closely related Markov chain whose state space is $\Z_k$.
Since the space $\Z_k$ is typically of a fixed dimension, the hope is that chains that move in $\Z_k$ can be analyzed using well-established tools such as drift and minorization or functional inequalities.

\begin{lemma} \label{lem:Pk-convergence}
	Let~$k$ be in $\mathsf{K}$.
	Suppose that $\SF_k$ is countably generated, $\Phi_k P_k = \Phi_k$, and the chain associated with $P_k$ is $\varphi$-irreducible.
	Then, in each of the following situations, (ii) in \ref{H1} holds with $t_0 = 1$.
	\begin{enumerate}
		\item [(i)] $P_k$ defines a $\Phi_k$-a.e. geometrically ergodic chain that is reversible with respect to $\Phi_k$.
		\item [(ii)] $P_k$ defines a deterministic-scan Gibbs chain with two components that is $\Phi_k$-a.e. geometrically ergodic.
		\item [(iii)] $P_k$ defines a deterministic-scan Gibbs chain, and there exists a $\Phi_k$-a.e. geometrically ergodic random-scan Gibbs chain based on the same set of conditional distributions.
	\end{enumerate}
\end{lemma}
\begin{proof}
	For (i), see Theorem 2.1 of \cite{roberts1997geometric} and Theorem 2 of \cite{roberts2001geometric}.
	For (ii), see Lemma 3.2 and Proposition 3.5 of \cite{qin2020convergence}.
	For (iii), see Theorem 3.1 of \cite{chlebicka2023solidarity} and invoke (i).
\end{proof}


In Section \ref{ssec:mixture}, we give an example of establishing and utilizing (ii) in \ref{H1} with $t_0 > 1$.






\section{Preliminaries}  \label{sec:l2}


Let $(\Y, \F, \omega)$ be a generic probability space.
Let $L^2(\omega)$ be the Hilbert space of real functions $f: \Y \to \mathbb{R}$ that are square integrable with respect to~$\omega$, with the inner product between two functions defined as
$
\langle f, g \rangle_{\omega} = \int_{\Y} f(y) g(y) \, \omega(\df y),
$
and the norm defined as $\|f\|_{\omega} = \sqrt{\langle f, f \rangle_{\omega}}$.
Denote by $L_0^2(\omega)$ the subspace of $L^2(\omega)$ that consists of functions~$f$ such that $\omega f := \langle f, \ind_{\Y} \rangle_{\omega} = 0$, where $\ind_{\Y}(y) = 1$ for $y \in \Y$.
A probability measure $\mu$ on $(\Y, \F)$ is said to be in $L_*^2(\omega)$ if $\df \mu/ \df \omega$ exists and is in $L^2(\omega)$. 
For two probability measures $\mu$ and $\nu$ in $L_*^2(\omega)$, define their $L^2$ distance by $\|\mu - \nu\|_{\omega} = \|\df\mu/\df \omega - \df \nu/\df \omega\|_{\omega}$.



Let $\Mtk: \Y \times \F \to [0,1]$ be an Mtk whose stationary distribution is~$\omega$.
For a probability measure~$\mu$ on~$\F$, define $\mu \Mtk^t(\cdot) = \int_{\Y} \mu(\df y) \Mtk^t(y, \cdot)$, where $\Mtk^t$ is the corresponding $t$-step Mtk.
We say $\Mtk$ is $L^2(\omega)$ geometrically convergent if there exist $\rho < 1$ and a function $C: L_*^2(\omega) \to [0,\infty)$ such that for $\mu \in L_*^2(\omega)$ and $t \geq 1$,
\begin{equation} \label{ine:geometric}
	\|\mu \Mtk^t - \omega\|_{\omega} \leq C(\mu) \rho^t.
\end{equation}
Let $\|\cdot\|_{\scriptsize\mbox{TV}}$ be the total variance distance between two probability measures.
$\Mtk$ is said to be $\omega$-a.e. geometrically ergodic if there exist $\rho < 1$ and $C: \Y \to [0, \infty)$ such that, for $\omega$-almost every $y \in \Y$ and $t \geq 1$,
$
	\|\Mtk^t(y,\cdot) - \omega(\cdot) \|_{\scriptsize\mbox{TV}} \leq C(y) \rho^t.
$
Results from \cite{roberts2001geometric} indicate that when $\F$ is countably generated, if the chain is $L^2(\omega)$ geometrically convergent, then it is $\omega$-a.e. geometrically ergodic; the converse holds if $\Mtk$ is reversible with respect to~$\omega$.
See also \cite{roberts1997geometric} and \cite{gallegos2023equivalences}.



The Mtk~$\Mtk$ can be understood as a linear operator on $L_0^2(\omega)$: for $f \in L_0^2(\omega)$, 
$
\Mtk f(\cdot) = \int_{\Y} \Mtk(\cdot, \df y) f(y).
$
One can use the Cauchy-Schwarz inequality to show that the $L^2$ norm of $\Mtk$, defined as
\[
\|\Mtk\|_{\omega} = \sup_{f \in L_0^2(\omega) \setminus \{0\}} \frac{\|\Mtk f\|_{\omega}}{\|f\|_{\omega}},
\]
is no greater than~1.
The operator norms of $\Mtk$ and its powers quantify the Markov chain's convergence rate, with smaller norms indicating faster convergence.
Indeed, if $s$ is a positive integer, then~\eqref{ine:geometric} holds for all $\mu \in L_*^2(\omega)$ and $t \geq 1$ with $\rho = \|\Mtk ^s\|_{\omega}^{1/s}$ and some $C(\cdot)$.
See Theorem 2.1 of \cite{roberts1997geometric} for more details on the interpretation of $\|\Mtk\|_{\omega}$.


The bounded operator~$\Mtk $ has a unique adjoint $\Mtk ^*$.
%For $f, g \in L_0^2(\omega)$, $\langle \Mtk f, g \rangle_{\omega} = \langle f, \Mtk ^*g \rangle_{\omega}$.
It is well-known that $\|\Mtk \|_{\omega}^2 = \|\Mtk ^*\|_{\omega}^2 = \|\Mtk  \Mtk ^*\|_{\omega} = \|\Mtk ^*\Mtk \|_{\omega}$.
The Mtk $\Mtk$ is reversible with respect to $\omega$ if and only if the operator $\Mtk$ is self-adjoint, i.e., $\Mtk  = \Mtk ^*$.
The operator~$\Mtk $ is positive semi-definite if it is self-adjoint, and $\langle \Mtk f, f \rangle_{\omega} \geq 0$ for $f \in L_0^2(\omega)$.

When $\Mtk$ is self-adjoint, its spectral gap is defined to be
\[
\mbox{Gap}_{\omega} (\Mtk) = 1 - \sup_{f \in L_0^2(\omega) \setminus \{0\}} \frac{\langle f, \Mtk f \rangle_{\omega}}{\|f\|_{\omega}^2}.
\]
Note that $	\mbox{Gap}_{\omega} (\Mtk) \geq 1 - \|\Mtk\|_{\omega} \geq 0$.
If $\Mtk$ is positive semi-definite, $\|\Mtk \|_{\omega} = 1 - \mbox{Gap}_{\omega}(\Mtk)$.



\section{Convergence Analysis} \label{sec:main}

\subsection{Markov chain decomposition}

This subsection describes the main probabilistic tool for proving Theorem~\ref{thm:main}.

Again, let $(\Y, \F, \omega)$ be a probability space.
Suppose that $\Y$ can be partitioned into a collection of disjoint subsets, $(\Y_k)_{k \in \mathsf{K}}$.
For this subsection, allow~$\mathsf{K}$ to be countably infinite.
Assume that $\omega(\Y_k) > 0$ for each~$k$.
\cite{caracciolo1992two} proposed a framework for analyzing a Markov chain moving in~$\Y$ by decomposing its dynamic into local movements within a subset $\Y_k$ and global movements across the disjoint subsets.
The key technical result, published in \cite{madras2002markov}, is stated for reversible chains \citep[see also, e.g.,][]{guan2007small,woodard2009conditions}.
Here, it is extended to a possibly non-reversible setting.


For $k \in \mathsf{K}$, let $\F_k$ be the restriction of~$\F$ on $\Y_k$, and let $\omega_k(\mathsf{B}) = \omega(\mathsf{B})/\omega(\Y_k)$ for $\mathsf{B} \in \F_k$.
Let $\bar{\omega}(\{k\}) = \omega(\Y_k)$ for $k \in \mathsf{K}$.
{ For an Mtk $\mtkfont{S}: \Y \times \F \to [0,1]$ such that $\omega \mtkfont{S} = \omega$, let
$\bar{\mtkfont{S}}$ be an Mtk on the discrete space $\mathsf{K}$ such that, for $k, k' \in \mathsf{K}$, 
\[
\bar{\mtkfont{S}}(k, \{k'\}) = \frac{1}{\omega(\Y_k)} \langle \ind_{\Y_k}, S \ind_{\Y_{k'}} \rangle_{\omega} = \frac{1}{\omega(\Y_k)} \int_{\Y_k} \omega(\df y) \, \mtkfont{S}(y, \Y_{k'}).
\] 
Then $\bar{\omega} \bar{\mtkfont{S}} = \bar{\omega}$.
It can be checked that $\bar{\mtkfont{S}}$ defines a self-adjoint (resp. positive semi-definite) operator on~$L_0^2(\bar{\omega})$ whenever $\mtkfont{S}$ is self-adjoint (resp. positive semi-definite). }
In the same vein, define the Mtk $\overline{\mtkfont{S}^*\mtkfont{S}}$ on $\mathsf{K}$, which takes the form
\[
\overline{\mtkfont{S}^*\mtkfont{S}}(k,\{k'\}) = \frac{1}{\omega(\Y_k)} \langle \ind_{\Y_k}, S^* S \ind_{\Y_{k'}} \rangle_{\omega} = \frac{1}{\omega(\Y_k)} \int_{\Y} \omega(\df y) \, \mtkfont{S}(y, \Y_k) \mtkfont{S}(y, \Y_{k'}).
\]
As long as $\omega \mtkfont{S} = \omega$, $\overline{\mtkfont{S}^*\mtkfont{S}}$ defines a positive semi-definite operator on~$L_0^2(\bar{\omega})$.

%	We first state \pcite{caracciolo1992two} original result, up to minor modifications.
%	\begin{lemma} \citep[][Theorem A.1]{madras2002markov} \label{lem:madras}
	%		Let $\Mtk $ and $\mtkfont{S}$ be Mtks that are reversible with respect to $\omega$, and assume that $\mtkfont{S}$ is positive semi-definite.
	%		Suppose that for $k \in \mathsf{K}$, there exists an Mtk $\Mtk_k : \Y_k \times \F_k \to [0,1]$ such that $\omega_k T_k = \omega_k$.
	%		Assume further that there exists $c \in [0,1]$ such that $\Mtk (y,B) \geq c \Mtk _k(y,B)$ for $k \in \mathsf{K}$, $y \in \Y_k$ and $B \in \F_k$.
	%		Then
	%		\begin{equation} \nonumber
		%			\mbox{Gap}_{\omega}(\mtkfont{S}^{1/2} \Mtk \mtkfont{S}^{1/2}) \geq c \inf_{k \in \mathsf{K}} \mbox{Gap}_{\omega_k}(\Mtk_k) \mbox{Gap}_{\bar{\omega}} (\bar{\mtkfont{S}} ).
		%		\end{equation}
	%		In particular, if $\Mtk$ is positive semi-definite, and $\mtkfont{S} = \Mtk$, then
	%		\[
	%		1 - \|\Mtk\|_{\omega}^2 = \mbox{Gap}_{\omega}(\Mtk^2) \geq c \inf_{k \in \mathsf{K}} \mbox{Gap}_{\omega_k}(\Mtk_k) \mbox{Gap}_{\bar{\omega}} ( \bar{\Mtk} ).
	%		\]
	%	\end{lemma}
%	\begin{remark}
	%			The assumptions involving the $\Mtk_k$'s are slightly stronger than those stated in \cite{madras2002markov}. 
	%			However, this modification makes the lemma easier to apply to the settings we are interested in.
	%	\end{remark}
%	
%	Intuitively, Lemma~\ref{lem:madras} relates the norm of $\Mtk$ to its global and local behavior, characterized by $\bar{\Mtk}$ and the $\Mtk_k$'s, respectively.
%	
%	The bound on $\|\Mtk\|_{\omega}$ in Lemma \ref{lem:madras} is applicable only if the operator $\Mtk$ is positive semi-definite (and thus self-adjoint).
%	Assume now that the Mtks $\Mtk$ and $\mtkfont{S}$ are not necessarily reversible, but we still have $\omega \mtkfont{S} = \omega \Mtk = \omega$.
%	Define the Mtk $\overline{\mtkfont{S}^*\mtkfont{S}}$ on $\mathsf{K}$, which takes the form
%	\[
%	\overline{\mtkfont{S}^*\mtkfont{S}}(k,\{k'\}) = \frac{1}{\omega(\Y_k)} \langle \ind_{\Y_k}, S^* S \ind_{\Y_{k'}} \rangle_{\omega} = \frac{1}{\omega(\Y_k)} \int_{\Y} \omega(\df y) \, \mtkfont{S}(y, \Y_k) \mtkfont{S}(y, \Y_{k'}).
%	\]
%	Then $\overline{\mtkfont{S}^*\mtkfont{S}}$ is reversible with respect to $\bar{\omega}$.
%	In fact, $\overline{\mtkfont{S}^*\mtkfont{S}}$ defines a positive semi-definite operator on~$L_0^2(\bar{\omega})$.


Below is the key technical lemma of this subsection. 

\begin{lemma} \label{lem:decomposition}
	Let $\Mtk $ and $\mtkfont{S}$ be Mtks such that $\omega \Mtk  = \omega \mtkfont{S} = \omega$.
	Suppose that for $k \in \mathsf{K}$, there exists an Mtk $\Mtk_k : \Y_k \times \F_k \to [0,1]$ such that $\omega_k \Mtk _k = \omega_k$.
	Assume further that there exists $c \in [0,1]$ such that $\Mtk (y,\mathsf{B}) \geq c \Mtk _k(y,\mathsf{B})$ for $k \in \mathsf{K}$, $y \in \Y_k$ and $\mathsf{B} \in \F_k$.
	Then
	\begin{equation} \label{ine:decomposition}
		1 - \|\Mtk \mtkfont{S}^*\|_{\omega}^2 \geq c^2 \left(1 - \sup_{k \in \mathsf{K}} \|\Mtk _k\|_{\omega_k}^2 \right) \mbox{Gap}_{\bar{\omega}} (\overline{\mtkfont{S}^*\mtkfont{S}} ).
	\end{equation}
	In particular, if furthermore $\mtkfont{S} = \Mtk$, then
	\[
	1 - \|\Mtk\|_{\omega}^4 \geq c^2 \left(1 - \sup_{k \in \mathsf{K}} \|\Mtk _k\|_{\omega_k}^2 \right) \mbox{Gap}_{\bar{\omega}} ( \overline{\mtkfont{\Mtk}^*\mtkfont{\Mtk}} ).
	\]
\end{lemma}

{
\begin{remark} \label{rem:madras}
	Lemma \ref{lem:decomposition} extends Theorem A.1 of \cite{madras2002markov}, originally formulated by \cite{caracciolo1992two}.
	From \pcite{caracciolo1992two} result, it can be deduced that, if, in addition to the assumptions in Lemma \ref{lem:decomposition}, $\Mtk$ and $\mtkfont{S}$ are reversible with respect to~$\omega$, $\Mtk_k$ is reversible with respect to $\omega_k$ for $k \in \mathsf{K}$, and the operator $\mtkfont{S}$ is positive semi-definite, then
	\[
	\mbox{Gap}_{\omega}(\mtkfont{S}^{1/2} \Mtk \mtkfont{S}^{1/2}) \geq c \inf_{k \in \mathsf{K}} \mbox{Gap}_{\omega_k}(\Mtk_k) \mbox{Gap}_{\bar{\omega}} (\bar{\mtkfont{S}}).
	\]
	In particular, if furthermore $\mtkfont{S} = \Mtk$, then
	\[
	1 - \|\Mtk\|_{\omega}^2 = \mbox{Gap}_{\omega}(\Mtk^2) \geq c \inf_{k \in \mathsf{K}} \mbox{Gap}_{\omega_k}(\Mtk_k) \mbox{Gap}_{\bar{\omega}} ( \bar{\Mtk} ).
	\]
%	For the sake of simplicity, the assumptions involving the $\Mtk_k$'s stated here are slightly stronger than those in \cite{madras2002markov}. 
\end{remark}

\begin{remark}
	The proof of Lemma \ref{lem:decomposition} is given in Section~\ref{app:decomposition} of \ref{s1}.
	It adopts the general idea of the proof of Theorem A.1 of \cite{madras2002markov}, with alterations made to tackle non-reversibility.
	Notably, \cite{madras2002markov} studied how a reversible $\Mtk$ acts on functions of the form $\mtkfont{S}^{1/2} f$ for $f \in L_0^2(\omega)$ by decomposing the Dirichlet form $\|\mtkfont{S}^{1/2}f\|_{\omega}^2 - \langle \mtkfont{S}^{1/2}f, \Mtk \mtkfont{S}^{1/2}f \rangle_{\omega}$ into local and global components.
	Here, we study how a possibly non-reversible $\Mtk$ acts on functions of the form $\mtkfont{S}^* f$ by decomposing $\|\mtkfont{S}^* f \|_{\omega}^2 - \|\Mtk \mtkfont{S}^* f\|_{\omega}^2$.
\end{remark}

\begin{remark} \label{rem:lower}
	Using standard techniques, it is straightforward to derive the following bound in the opposite direction of those given in Lemma \ref{lem:decomposition} and \cite{madras2002markov}:
	$$
	1 - \|\Mtk\|_{\omega}^2 \leq \mbox{Gap}_{\bar{\omega}}(\overline{\Mtk^* \Mtk}) \leq 1 - \|\bar{\mtkfont{T}}\|_{\bar{\omega}}^2.
	$$
	We provide a brief derivation at the end of Section \ref{app:decomposition} in \ref{s1}.
\end{remark}
}



\subsection{Geometric convergence of the trans-dimensional chain} \label{ssec:geometric}

Lemma \ref{lem:decomposition} can be used to construct an upper bound on the norm of $P^t$ for some $t \geq 1$, where~$P$ is the Mtk of the trans-dimensional chain defined in the Introduction.


{
Recall the definitions of $\bar{\mtkfont{S}}$ and $\overline{\mtkfont{S}^*\mtkfont{S}}$, and consider letting $(\Y,\F,\omega) = (\X, \SF, \Pi)$ and $\mtkfont{S} = P$.
Then $\bar{P}$ is defined as in \ref{H2}, and
\[
\begin{aligned}
	\overline{P^*P}(k,\{k'\}) &= \frac{1}{\Psi_k(\Z_k)} \sum_{k'' \in \mathsf{K}} \int_{\Z_{k''}} \Psi_{k''}(\df z) P((k'',z), \{k\} \times \Z_k ) P((k'',z), \{k'\} \times \Z_{k'}).
\end{aligned}
\]
These two Mtks describe the between-model movements of the trans-dimensional chain.
$\bar{P}(k,\{k'\})$ can be understood as the average probability of moving from model~$k$ to model~$k'$.
$\overline{P^*P}(k,\{k'\})$ is similar, but with $P$ replaced by $P^*P$. 
Indeed, under mild conditions, $P^*$ and thus $P^*P$ can be seen as Mtks that leave $\Pi$ invariant \citep{paulin2015concentration,choi2020metropolis}, and one can show that
$
\overline{P^*P}(k,\{k'\}) = \int_{\Z_k} \Phi_k(\df z) P^*P((k,z), \{k'\} \times \Z_{k'}) .
$
$\overline{P^*P}$ is called the ``multiplicative reversibilization" of $P$. 
Multiplicative reversibilizations are commonly investigated for non-reversible chains since at least \cite{fill1991eigenvalue}.
If $P$ defines a self adjoint (resp. positive semi-positive) operator on $L_0^2(\Pi)$, then $\bar{P}$ defines a self adjoint (resp. positive semi-positive) operator on $L_0^2(\bar{\Pi})$, where $\bar{\Pi}(\{k\}) = \Psi_k(\Z_k)/\sum_{k' \in \mathsf{K}} \Psi_{k'}(\Z_{k'})$ for $k \in \mathsf{K}$.
On the other hand, $\overline{P^*P}$ always defines a positive semi-definite operator on $L_0^2(\bar{\Pi})$.
}

We now provide a quantitative bound concerning the convergence rate of the trans-dimensional chain.

%	\begin{remark}
	%		If $P$ is reversible with respect to~$\Pi$, then
	%		\[
	%		\bar{P}(k,\{k'\}) = \int_{\Z_k} \Phi_k(\df z) P^2((k,z), \{k'\} \times \Z_{k'}).
	%		\]
	%		This is an average probability of moving from model~$k$ to model~$k'$ after two steps.
	%	\end{remark}

\begin{theorem} \label{thm:quantitative}
	Just for this theorem, allow $|\mathsf{K}|$ to be countably infinite.
	Suppose that, for each $k \in \mathsf{K}$, there exists an Mtk $P_k: \Z_k \times \A_k \to [0,1]$ such that $\Phi_k P_k = \Phi_k$.
	Suppose further that, for $k \in \mathsf{K}$, there exists $c_k > 0$ such that $P((k,z), \{k\} \times \mathsf{A}) \geq c_k P_k(z, \mathsf{A})$ for $z \in \Z_k$ and $\mathsf{A} \in \A_k$.
	Then, for any positive integer~$t$,
	\begin{equation} \label{ine:quantitative-1}
		1 - \|P^t\|_{\Pi}^4 \geq \left( \inf_{k \in \mathsf{K}} c_k^t \right)^2 \left( 1 - \sup_{k \in \mathsf{K}} \|P_k^t \|_{\Phi_k}^2 \right) \mbox{Gap}_{\bar{\Pi}}(\overline{P^*P}) .
	\end{equation}
	{If, furthermore, $P$ defines a positive semi-definite operator on $L_0^2(\Pi)$ and $P_k$ is reversible with respect to $\Phi_k$ for $k \in \mathsf{K}$, then there is the simpler bound
	\begin{equation} \label{ine:quantitative-2}
		1 - \|P\|_{\Pi}^2 \geq \left( \inf_{k \in \mathsf{K}} c_k \right) \left[ \inf_{k \in \mathsf{K}} \mbox{Gap}_{\Phi_k}(P_k) \right] \mbox{Gap}_{\bar{\Pi}}(\bar{P}).
	\end{equation}}
\end{theorem}

\begin{proof}
	We will establish \eqref{ine:quantitative-1} using Lemma \ref{lem:decomposition}; \eqref{ine:quantitative-2} can be established in a similar fashion using the original Markov chain decomposition result in Remark \ref{rem:madras}.
	
	Fix a positive integer~$t$.
	In Lemma~\ref{lem:decomposition}, take $(\Y,\F,\omega) = (\X,\SF,\Pi)$, $\Mtk = P^t$ and $\mtkfont{S} = P$.
	For $k \in \mathsf{K}$, let $\Y_k = \{k\} \times \Z_k$.
	Then $\F_k$ consists of sets of the form $\{k\} \times \mathsf{A}$, where $\mathsf{A} \in \A_k$, and $\omega_k(\{k\} \times \mathsf{A}) = \Phi_k(\mathsf{A})$ for $\mathsf{A} \in \A_k$.
	For $k \in \mathsf{K}$, $z \in \Z_k$, and $\mathsf{A} \in \A_k$, let
	$
	\Mtk_k((k,z), \{k\} \times \mathsf{A}) = P_k^t(z, \mathsf{A}).
	$
	Since $\Phi_k P_k = \Phi_k$, it holds that $\omega_k \Mtk_k = \omega_k$.
	Since $P((k,z), \{k\} \times \mathsf{A}) \geq c_k P_k(z,\mathsf{A})$ for $z \in \Z_k$ and $\mathsf{A} \in \mathcal{A}_k$, it holds that, for $(k,z) \in \Y_k$ and $\{k\} \times \mathsf{A} \in \F_k$,
	\[
	\Mtk((k,z), \{k\} \times \mathsf{A}) \geq c_k^t P_k^t(z, \mathsf{A}) \geq c \Mtk_k((k,z), \{k\} \times \mathsf{A}),
	\]
	where $c = \inf_{k \in \mathsf{K}} c_k^t$.
	Thus, the assumptions of Lemma~\ref{lem:decomposition} are satisfied.
	
	The next step is identifying the objects in \eqref{ine:decomposition}.
	Obviously, $\|\Mtk \mtkfont{S}^*\|_{\omega} = \|P^t P^*\|_{\Pi}$.
	Standard arguments show that $\|\Mtk_k\|_{\omega_k} = \|P_k^t\|_{\Phi_k}$.
	The distribution $\bar{\omega}$ corresponds to $\bar{\Pi}$, while the Mtk $\overline{\mtkfont{S}^* \mtkfont{S}}$ is $\overline{P^*P}$.
	% Since $\bar{\mtkfont{S}} = \bar{P}$ is positive definite, $\|\bar{\mtkfont{S}}\|_{\bar{\omega}}$ is the second largest eigenvalue of $M_P$.
	Then, by Lemma~\ref{lem:decomposition},
	\begin{equation} \nonumber
		1 - \|P^t P^*\|_{\Pi}^2 \geq c^2 \left( 1 - \sup_{k \in \mathsf{K}} \|P_k^t \|_{\Phi_k}^2 \right) \mbox{Gap}_{\bar{\Pi}}(\overline{P^*P}). 
	\end{equation}
	Finally, note that
	\[
	\|P^t\|_{\Pi}^2 = \|P^t P^{*t}\|_{\Pi} \leq \|P^t P^*\|_{\Pi} \|P^{* t-1}\|_{\Pi} = \|P^t P^*\|_{\Pi} \|P^{t-1}\|_{\Pi}  \leq \|P^t P^*\|_{\Pi}.
	\]
	The desired result then follows.
\end{proof}


{
Theorem \ref{thm:quantitative} connects the convergence properties of the trans-dimensional chain, quantified by $\|P\|_{\Pi}$, to the convergence properties of the within- and between-model movements, quantified by the $\|P_k\|_{\Phi_k}$'s and $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P})$ (or $\mbox{Gap}_{\bar{\Pi}}(P)$), respectively.
In Section \ref{ssec:toy}, we use a toy example to test the sharpness of the bounds in Theorem \ref{thm:quantitative}, and investigate how the within- and between-model components may affect $\|P\|_{\Pi}$ and its bound.

\begin{remark} \label{rem:lower-2}
	By Remark~\ref{rem:lower}, we also have $1 - \|P\|_{\Pi}^2 \leq \mbox{Gap}_{\bar{\Pi}}(\overline{P^*P})$, and if $P$ is reversible, $1 - \|P\|_{\Pi} \leq \mbox{Gap}_{\bar{\Pi}}(\bar{P})$.
	Thus, $\|P\|_{\Pi}$ is controlled by $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P})$ from above and below.
	In particular, combining \eqref{ine:quantitative-1} with the above yields
	\[
	\frac{1}{4} \left( \inf_{k \in \mathsf{K}} c_k \right)^2 \left( 1 - \sup_{k \in \mathsf{K}} \|P_k \|_{\Phi_k}^2 \right) \leq \frac{1 - \|P\|_{\Pi}}{\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P})} \leq 1.
	\]
	Similarly, if $P$ is positive semi-definite and $P_k$ is reversible for each~$k$,
	\[
	\frac{1}{2} \left( \inf_{k \in \mathsf{K}} c_k \right) \left[ \inf_{k \in \mathsf{K}} \mbox{Gap}_{\Phi_k}(P_k) \right] \leq \frac{1 - \|P\|_{\Pi}}{\mbox{Gap}_{\bar{\Pi}}(\bar{P}) } \leq 1.
	\]
\end{remark}

}

Quantities such as $\|\overline{P^*P}\|_{\Pi}$ and $\|P_k\|_{\Phi_k}$ may be difficult to compute in practice.
However, when $|\mathsf{K}| < \infty$, Theorem~\ref{thm:quantitative} immediately yields Theorem~\ref{thm:main}, which is stated again below:

\noindent{\bf Theorem \ref{thm:main}.} 
{\it
	Assume that \ref{H1} and \ref{H2} hold.
	Then $\|P^{t_0}\|_{\Pi} < 1$, and~$P$ is $L^2(\Pi)$ geometrically convergent.
}

\begin{proof}
	By Theorem~\ref{thm:quantitative}, it suffices to show that 
	\[
	\left( \min_{k \in \mathsf{K}} c_k^{t_0} \right)^2 \left( 1 - \max_{k \in \mathsf{K}} \|P_k^{t_0} \|_{\Phi_k}^2 \right) \mbox{Gap}_{\bar{\Pi}}(\overline{P^*P}) > 0.
	\]
	
	By (iii) in \ref{H1}, $\min_{k \in \mathsf{K}} c_k^{t_0} > 0$.
	By (ii) in \ref{H1}, $\max_{k \in \mathsf{K}} \|P_k^{t_0} \|_{\Phi_k}^2 < 1$.
	
	{
	It remains to show that $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P}) > 0$.
	Assume the opposite, i.e., $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P}) = 0$.
	Because $\mathsf{K}$ is finite and $\overline{P^*P}$ is reversible, this implies that the largest eigenvalue of $\overline{P^*P}$ is 1.
	It then follows that $\overline{P^*P}$ is reducible \citep[][Theorem 3.11]{hairer2006ergodic}.
	By (iii) in \ref{H1}, for $k, k' \in \mathsf{K}$,
	\[
	\begin{aligned}
		\overline{P^*P}(k, \{k'\}) &\geq \int_{\Z_k} \Phi_k(\df z) P((k,z), \{k\} \times \Z_k ) P((k,z), \{k'\} \times \Z_{k'}) \geq c_k \bar{P}(k,\{k'\}).
	\end{aligned}
	\]
	So $\bar{P}$ must be reducible as well.
	But this contradicts with \ref{H2}.
	Hence, $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P}) > 0$.
	}
	
	%		Thus, there exists $A_0 \subset \mathsf{K}$ such that $A_0$ and $A_0^c$ are both non-empty, and $\overline{P^*P}(k, \{k'\}) = 0$ whenever $k \in A_0$ and $k' \in A_0^c$.
	%		By (iii) in \ref{H1}, for $(k, k') \in \mathsf{K} \times \mathsf{K}$, 
	%		\[
	%		\begin{aligned}
		%			\Psi_k(\Z_k) \overline{P^*P}(k, \{k'\}) &\geq \int_{\Z_k} \Psi_k(\df z) P((k,z), \{k\} \times \Z_k ) P((k,z), \{k'\} \times \Z_{k'}) \\
		%			&\geq c_k \int_{\Z_k} \Psi_k(\df z) P((k,z), \{k'\} \times \Z_{k'}).
		%		\end{aligned}
	%		\]
	%		Then $\int_{\Z_k} \Psi_k(\df z) P((k,z), \{k'\} \times \Z_{k'}) = 0$ if $k \in A_0$ and $k' \in A_0^c$.
	%		We will show that, for any positive integer~$t$, $k \in A_0$, and $k' \in A_0^c$,
	%		\begin{equation} \label{ine:PsikP2}
		%		\int_{\Z_k} \Psi_k(\df z) P^t((k,z), \{k'\} \times \Z_{k'}) = 0,
		%		\end{equation}
	%		contradicting \ref{H2}.
	%		It was already shown that \eqref{ine:PsikP2} holds when $t = 1$.
	%		Assume that it holds for some $t \geq 1$; we now show that it holds for $t+1$.
	%		For $k \in A_0$ and $k' \in A_0^c$,
	%		\[
	%		\begin{aligned}
		%			&\int_{\Z_k} \Psi_k(\df z) P^{t+1}((k,z), \{k'\} \times \Z_{k'}) \\
		%			=& \sum_{k'' \in A_0} \int_{\Z_k} \Psi_k(\df z) \int_{Z_{k''}} P^t((k,z), (k'', \df z'')) P((k'',z''), \{k'\} \times \Z_{k'}) \\
		%			\leq& \sum_{k'' \in A_0} \int_{Z_{k''}} \sum_{k''' \in \mathsf{K}} \int_{\Z_{k'''}} \Psi_{k'''}(\df z''') P^t((k''',z'''), (k'', \df z'')) P((k'',z''), \{k'\} \times \Z_{k'}) \\
		%			=& \sum_{k'' \in A_0} \int_{Z_{k''}} \Psi_{k''}(\df z'') P((k'',z''), \{k'\} \times \Z_{k'}) \\
		%			=& 0.
		%		\end{aligned}
	%		\]
	%		Hence, \eqref{ine:PsikP2} holds for each $t$, leading to a contradiction.
	%		In conclusion, it must hold that $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P}) > 0$.
	%		This concludes the proof.
	
	
	
\end{proof}

Theorem~\ref{thm:main} will be used to establish geometric convergence for two practical examples in Section~\ref{sec:examples} and another one in \ref{s2}.








\section{Examples} \label{sec:examples}

This section contains a toy example and two practical problems concerning variable selection and mixture models respectively.
For a third practical example concerning autoregression, see \ref{s2}.

{
\subsection{A toy chain} \label{ssec:toy}
}

We first use a toy algorithm to test the sharpness of the quantitative bounds in Theorem~\ref{thm:quantitative}.

% \subsubsection{The transition law}

Let $k_{\scriptsize\mbox{max}}$ and $n$ be positive integers.
Let $\mathsf{K} = \{1,\dots,k_{\scriptsize\mbox{max}}\}$.
Consider a simple scenario where all the $\Psi_k$'s are the same.
To be specific, for $k = 1,\dots,k_{\scriptsize\mbox{max}}$, let $\Z_k = \{1,\dots,n\}$, and let $\Psi_k$ be the counting measure on $\Z_k$.
Then $\Pi$ is the uniform distribution on $\X = \bigcup_{k \in \mathsf{K}} \{k\} \times \Z_k$.

We consider a type of MCMC algorithm targeting $\Pi$.
Given the current state $(k,z) \in \X$, the algorithm either makes a local or a global move, each with probability $1/2$.
A local move sends the underlying Markov chain to $(k,z')$ where $z' \in \Z_k$, and a global move sends the chain to $(k',z)$ where $k' \in \mathsf{K}$.
We consider three types of local moves and two types of global moves.
The three types of local moves are ``fast," ``slow," and ``varied."
In a fast local move, the chain randomly and uniformly selects $z' \in \Z_k$ to move to.
In a slow local move, the chain can either stay in place with probability $1/2$, or move to one of the within-model neighbors.
Within a model~$k$, two states $z$ and $z'$ are neighbors if they differ by~1, or if one of them is~1 and the other is~$n$.
In a varied local move, the movement of the chain varies with the current model~$k$.
To be precise, the chain stays in place with probability $1-1/k$, and move randomly and uniformly across $\Z_k$ with probability $1/k$.
The two types of global moves are ``fast" and ``slow."
In a fast global move, the chain randomly and uniformly selects a new model $k' \in \mathsf{K}$ to move to.
In a slow global move, the chain can stay in place with probability $1/2$, or move to one of the neighboring models.
Two models $k$ and $k'$ are neighbors if they differ by~1, or if one of them is~1 and the other is $k_{\scriptsize\mbox{max}}$.
Each combination of local and global move types gives rise to a concrete algorithm, and we may define six algorithms in this manner.


For a given algorithm, let $P$ be the Mtk of the underlying chain, and let $P_k$ be the Mtk associated with the local movement within model~$k$.
One can check that, for each of the six algorithms, $P$ is positive semi-definite, and so are the $P_k$'s.
The local and global behavior of~$P$ is summarized as follows.
\begin{itemize}
	\item When the local move type is fast, $\|P_k\|_{\Phi_k} = 0$.
	When the local move type is slow, $\|P_k\|_{\Phi_k}$ is a function of~$n$, and it goes to~1 as $n \to \infty$.
	When the local move is varied, $\|P_k\|_{\Phi_k} = 1-1/k$.
	\item When the global move type is fast, $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P}) = 3/4$ and $\mbox{Gap}_{\bar{\Pi}}(\bar{P}) = 1/2$.
	When the local move type is slow, $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P})$ is a function of $k_{\scriptsize\mbox{max}}$, and it goes to 0 as $k_{\scriptsize\mbox{max}} \to \infty$; the same goes for $\mbox{Gap}_{\bar{\Pi}}(\bar{P})$.
\end{itemize}


\begin{figure}
	\begin{center}
		\includegraphics[width=\textwidth]{numeric}
		\caption{Performance of the quantitative bounds for the toy chain.
		Left: $(1-\|P\|_{\Pi})/(1-\|P\|_{\Pi}^{\dagger})$ for various values of~$n$ when $k_{\scriptsize\mbox{max}} = 15$.
		Middle: $(1-\|P\|_{\Pi})/(1-\|P\|_{\Pi}^{\dagger})$ for various values of $k_{\scriptsize\mbox{max}}$ when $n = 15$.
		Right: $(1-\|P\|_{\Pi}^{\dagger})/(1-\|P\|_{\Pi}^{\ddagger})$ for various values of $k_{\scriptsize\mbox{max}}$ when $n = 15$.
		} \label{fig:numeric}
	\end{center}
\end{figure}

Using Theorem \ref{thm:quantitative}, we may construct upper bounds on $\|P\|_{\Pi}$.
The bound derived from \eqref{ine:quantitative-1} with $c_k = 1/2$ and $t=1$ will be denoted by $\|P\|_{\Pi}^{\dagger}$, while the bound derived from \eqref{ine:quantitative-2}, which exploits reversibility, is denoted by $\|P\|_{\Pi}^{\ddagger}$.

Since $\X$ is finite, the true value of $\|P\|_{\Pi}$ can be computed.
We test the performance of the bounds through numerical simulation, and the results are given in Figure \ref{fig:numeric}.
Table \ref{tab:bound} loosely summarizes how the local and global move types affect the sharpness of $\|P\|_{\Pi}^{\dagger}$.
The bound $\|P\|_{\Pi}^{\ddagger}$ is sharper than $\|P\|_{\Pi}^{\dagger}$, but the two bounds are comparable.


\begin{table}
	\caption{Performance of the bound \eqref{ine:quantitative-1} for the toy chain} \label{tab:bound}
	\begin{center}
		\begin{tabular}{l|lll}
			\hline
			& Locally fast & Locally slow & Locally varied \\
			\hline
			Globally fast & well behaved & well behaved & deteriorates slowly as $k_{\scriptsize\mbox{max}} \to \infty$ \\
			Globally slow & well behaved & situation dependent & deteriorates as $k_{\scriptsize\mbox{max}} \to \infty$ \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

%
%
%Let $\mathsf{K} = \{1,\dots, k_{\scriptsize\mbox{max}}\}$, where $k_{\scriptsize\mbox{max}}$ is a positive integer.
%For $k \in \mathsf{K}$, let $\Z_k = \mathbb{Z}/(k\mathbb{Z}) =  \{\bar{1},\dots,\bar{k}\}$ be the additive group of congruence classes modulo $k$, and let $\Psi_k$ be the counting measure on $\Z_k$.
%Then $\Pi$, as given by \eqref{eq:Pi}, gives the uniform distribution on $\X = \bigcup_{k \in \mathsf{K}} \{k\} \times \Z_k$.
%
%Given $k \in \mathsf{K}$, to sample from $\Phi_k$, which is the uniform distribution on $\Z_k$, we consider two schemes.
%Given the current state, the next state $z'$ is drawn with the following probabilities:
%\[
%\begin{aligned}
%	&\text{Scheme 1: } P_{k,(1)}(z,\{z'\}) = \frac{1}{2} \ind_{\{z\}}(z') + \frac{1}{4} \ind_{\{z-\bar{1}\}}(z') + \frac{1}{4} \ind_{\{z+\bar{1}\}}(z'). \\
%	&\text{Scheme 2: } P_{k,(2)}(z,\{z'\}) = \frac{1}{\lceil k/2 \rceil+1} \sum_{j=0}^{\lceil k/2 \rceil} \ind_{\{z + \bar{j}\}}(z'),
%\end{aligned}
%\]
%where $\lceil \cdot \rceil$ is the ceiling function.
%It can be checked that both Mtks leave $\Phi_k$ invariant, but only $P_{k,(1)}$ is reversible for an arbitrary~$k$.
%%	When $k$ is large, we expect Scheme 1 to produce a chain that converges slowly, and Scheme 2, which can make larger leaps, to produce a chain that converges rapidly.
%
%Based on a given scheme, we may define a trans-dimensional MCMC algorithm targeting~$\Pi$.
%Given the current state $(k,z) \in \X$, the trans-dimensional algorithm associated with Scheme $s$, $s \in \{1,2\}$, randomly performs one of three move types, code-named U (update), B (birth), and D (death).
%The probabilities of choosing the moves are, respectively, $q_U(k) = 1/2 + \ind_{\{1\}}(k)/4 + \ind_{\{k_{\scriptsize\mbox{max}}\}}(k)/4$, $q_B(k) = 1/4 - \ind_{\{k_{\scriptsize\mbox{max}}\}}/4$, and $q_D(k) = 1/4 - \ind_{\{1\}}(k)/4$.
%The three move types are defined as follows:
%\begin{itemize}
%	\item {\it U move}: Draw $z'$ according to $P_{k,(s)}(z,\cdot)$.
%	Set the new state to $(k,z')$.
%	\item {\it B move}: With probability $1/2$, set the new state to $(k+1,z)$; otherwise, keep the old state.
%	This move type is only available when $k < k_{\scriptsize\mbox{max}}$.
%	\item {\it D move}: When $z \neq \bar{k}$, set the new state to $(k-1,z)$ with probability $1/2$, and otherwise keep the old state.
%	If $z = \bar{k}$, keep the old state.
%	This move type is only available when $k > 1$.
%\end{itemize}
%
%\subsubsection{Convergence bounds}
%
%Let $P_{(s)}$ be the Mtk of the trans-dimensional chain associated with Scheme~$s$.
%Then $\Pi P_{(s)} = \Pi$ for $s \in \{1,2\}$.
%Out of $P_{(1)}$ and $P_{(2)}$, only $P_{(1)}$ is always reversible with respect to $\Pi$.
%Since for $(k,z) \in \X$, $P_{(1)}((k,z), \{(k,z)\}) \geq 1/2$, $P_{(1)}$ defines a positive semi-definite operator on $L_0^2(\Pi)$.
%
%We now apply Theorem \ref{thm:quantitative} to $P_{(1)}$ and $P_{(2)}$ by taking $P = P_{(s)}$, $c_k = 1/2$, $P_k = P_{k,(s)}$, and $t = 1$.
%There are two bounds given in Theorem \ref{thm:quantitative}.
%The bound \eqref{ine:quantitative-2} can only be applied if $P$ is positive semi-definite --- so only to $P_{(1)}$.
%It gives an upper bound on $\|P_{(1)}\|_{\Pi}$, which we will denote by $\|P_{(1)}\|_{\Pi}^{\dagger}$.
%On the other hand, \eqref{ine:quantitative-1} can be applied to both Mtks.
%It gives upper bounds on $\|P_{(1)}\|_{\Pi}$ and $\|P_{(2)}\|_{\Pi}$, which will be denoted by $\|P_{(1)}\|_{\Pi}^{\ddagger}$ and $\|P_{(2)}\|_{\Pi}^{\ddagger}$, respectively.
%The bounds can be compared to the true values of $\|P_{(s)}\|_{\Pi}$, $s \in \{1,2\}$, which can be accurately computed due to how small $\X$ is.
%
%
%\begin{table} \caption{True values of $1-\|P_{(s)}\|_{\Pi}$ and their bounds} \label{tab:truth}
%	\begin{center}
%		\begin{tabular}{c|cccc}
%			\hline
%			$k_{\scriptsize\mbox{max}}$ & 10 &  20 &  30 &  40  \\
%			\hline
%			$1-\|P_{(1)}\|_{\Pi}$ & $1.4 \times 10^{-2}$  & $3.8 \times 10^{-3}$ & $1.7 \times 10^{-3}$ & $1.0 \times 10^{-3}$ \\
%			$1-\|P_{(2)}\|_{\Pi}$ & $1.4 \times 10^{-2}$ &  $4.1 \times 10^{-3}$ & $1.9 \times 10^{-3}$ & $1.1 \times 10^{-3}$ \\
%			\hline
%			$1-\|P_{(1)}\|_{\Pi}^{\dagger}$ & $3.6 \times 10^{-4}$ & $2.5 \times 10^{-5}$ & $5.2 \times 10^{-6}$ & $1.7 \times 10^{-6}$ \\
%			$1-\|P_{(1)}\|_{\Pi}^{\ddagger}$ & $3.4 \times 10^{-4}$ & $2.5 \times 10^{-5}$ & $5.2 \times 10^{-6}$ & $1.7 \times 10^{-6}$ \\
%			$1-\|P_{(2)}\|_{\Pi}^{\ddagger}$ & $1.4 \times 10^{-3}$ & $3.5 \times 10^{-4}$ & $1.5 \times 10^{-4}$ & $8.6 \times 10^{-5}$ \\
%			\hline
%		\end{tabular}
%	\end{center}
%\end{table}
%
%\begin{figure}
%	\begin{center}
%		\includegraphics[width=0.7\textwidth]{rjtoy} \caption{$\sqrt{(1-\text{truth})/(1-\text{bound})}$ plotted against $k_{\scriptsize\mbox{max}}$.
%			(1a): $\text{truth} = \|P_{(1)}\|_{\Pi}$, $\text{bound} = \|P_{(1)}\|_{\Pi}^{\dagger}$; (1b): $\text{truth} = \|P_{(1)}\|_{\Pi}$, $\text{bound} = \|P_{(1)}\|_{\Pi}^{\ddagger}$; (2): $\text{truth} = \|P_{(2)}\|_{\Pi}$, $\text{bound} = \|P_{(2)}\|_{\Pi}^{\ddagger}$.
%		} \label{fig:toy}
%	\end{center}
%\end{figure}
%
%Table \ref{tab:truth} contains the true values of $1-\|P_{(1)}\|_{\Pi}$ and $1-\|P_{(2)}\|_{\Pi}$ and their bounds when $k_{\scriptsize\mbox{max}}$ takes various values.
%The two norms are apparently very close to each other.
%In Figure \ref{fig:toy}, $\sqrt{(1-\text{truth})/(1-\text{bound})}$ is plotted against $k_{\scriptsize\mbox{max}}$ in three scenarios, where the truth is some $\|P_{(s)}\|_{\Pi}$, and the bound is either $\|P_{(s)}\|_{\Pi}^{\dagger}$ or $\|P_{(s)}\|_{\Pi}^{\ddagger}$.
%Note that if the bound is sharp, then $\sqrt{(1-\text{truth})/(1-\text{bound})}$ would be close to unity.
%We find that for the algorithm associated with Scheme 1, the bounds given by \eqref{ine:quantitative-2} and \eqref{ine:quantitative-1} both deteriorate as $k_{\scriptsize\mbox{max}}$ is increased, and the ratio $(1-\text{bound})/(1-\text{truth})$ seems to be roughly proportional to $k_{\scriptsize\mbox{max}}^2$.
%$\|P_{(1)}\|_{\Pi}^{\dagger}$, which exploits the reversibility of the algorithm, performs better than $\|P_{(1)}\|_{\Pi}^{\ddagger}$, but the difference is surprisingly small.
%On the other hand, $\|P_{(2)}\|_{\Pi}^{\ddagger}$, the bound for Scheme 2 does not deteriorate as $k_{\scriptsize\mbox{max}}$ is increased.
%
%
%
%\begin{table} \caption{Values of $1- \max_{k \in \mathsf{K}} \|P_{k,(s)}\|_{\Phi_k}$ and $\mbox{Gap}_{\bar{\Pi}}(\overline{P_{(s)}^*P_{(s)}})$} \label{tab:Pk}
%	\begin{center}
%		\begin{tabular}{c|cccc}
%			\hline
%			$k_{\scriptsize\mbox{max}}$ & 10 & 20 & 30 & 40 \\
%			\hline
%			$1- \max_{k \in \mathsf{K}} \|P_{k,(1)}\|_{\Phi_k}$ & $9.5 \times 10^{-2}$ & $2.4 \times 10^{-2}$ & $1.1 \times 10^{-2}$ & $6.2 \times 10^{-3}$ \\
%			$1- \max_{k \in \mathsf{K}} \|P_{k,(2)}\|_{\Phi_k}$ & $4.9 \times 10^{-1}$ & $4.3 \times 10^{-1}$ & $4.1 \times 10^{-1}$ & $3.9 \times 10^{-1}$ \\
%			\hline
%			$\mbox{Gap}_{\bar{\Pi}}(\overline{P_{(s)}^*P_{(s)}})^*$ & $3.0 \times 10^{-2}$ & $8.2 \times 10^{-3}$ & $3.8 \times 10^{-3}$ & $2.2 \times 10^{-3}$ \\
%			\hline
%		\end{tabular}
%	\end{center}
%\end{table}
%
%
%
%To understand how the local and global movements of a chain affect $\|P_{(s)}\|_{\Pi}$ and its bounds, we investigate $\max_{k \in \mathsf{K}}  \|P_{k,(s)}\|_{\Phi_k}$ and $\mbox{Gap}_{\bar{\Pi}}(\overline{P_{(s)}^*P_{(s)}})$ for different values of $k_{\scriptsize\mbox{max}}$.
%These quantities are given in Table \ref{tab:Pk}.
%Note that $\mbox{Gap}_{\bar{\Pi}}(\overline{P_{(s)}^*P_{(s)}})$ is the same for $s \in \{1,2\}$.
%Comparing Table \ref{tab:Pk} with Table \ref{tab:truth}, we see the following:



%	
%	To understand why $\|P_{(2)}\|_{\Pi}^{\ddagger}$ performs well for large $k_{\scriptsize\mbox{max}}$ while $\|P_{(1)}\|_{\Pi}^{\ddagger}$ does not, we note that $\|P_{(s)}\|_{\Pi}^{\ddagger}$ is close to unity if $\max_k \|P_{k,(s)}\|_{\Phi_k}$, where $1 \leq k \leq k_{\scriptsize\mbox{max}}$, is close to unity.
%	From Table \ref{tab:Pk}, we see that as $k$ increases, $\|P_{k,(1)}\|_{\Phi_k}$ appears to converge to~1, while $\|P_{k,(2)}\|_{\Phi_k}$ does not.
%	Intuitively, this makes sense since, when $k$ is large, in one step, the chain associated with $\|P_{k,(1)}\|_{\Phi_k}$ can only make small moves, while $\|P_{k,(2)}\|_{\Phi_k}$ is capable of making both small and large moves.
%	This discrepancy in the local behavior does not result in a large difference between $\|P_{(1)}\|_{\Pi}$ and $\|P_{(2)}\|_{\Pi}$, possibly because, in truth, the global behavior of the chains turns out to be the determining factor.
%	However, the discrepancy leads to a large difference in the bounds $\|P_{(1)}\|_{\Pi}^{\ddagger}$ and $\|P_{(2)}\|_{\Pi}^{\ddagger}$, which explicitly depend on $\max_k \|P_{k,(s)}\|_{\Phi_k}$.
%	
%	Finally, the effect of $\mbox{Gap}_{\bar{\Pi}}(\overline{P_{(s)}^*P_{(s)}})$ (which is the same for $s =1$ and $s=2$) on $1-\|P_{(s)}\|_{\Pi}$ and its bounds can be seen by comparing Tables \ref{tab:truth} and \ref{tab:P2bar}.







\subsection{Variable selection in Bayesian probit regression}

\subsubsection{The model}

% Consider now the problem of variable selection in a Bayesian binary regression model.

For $i = 1,\dots,n$, let $x_i = (x_{i,1}, \dots, x_{i,r})^{\top} \in \mathbb{R}^r$ be a known vector of predictors.
Let $Y_1, \dots, Y_n$ be independent binary responses, where  $Y_i$ follows a Bernoulli distribution with success probability $F(A+x_i^{\top} B)$, with $F(\cdot)$ being the cumulative distribution function of the standard normal distribution.
The scalar $A \in \mathbb{R}$ is an unknown intercept, while the vector $B= (B_1, \dots, B_r)^{\top} \in \mathbb{R}^r$ is an unknown regression coefficient.


To perform Bayesian variable selection, put a spike and slab prior on~$B$.
To be specific, let $K = (K_1, \dots, K_r)^{\top} \in \{0,1\}^r$.
Let $\mathsf{J}_K = \{j \in \{1,\dots,r\}: \, K_j = 1\}$.
Place a prior distribution on~$K$ that has probability mass function proportional to $p^{|\mathsf{J}_k|}$, where $p \in (0,1)$ is a hyperparameter.
Assume that given~$K$, the $B_j$'s are independent.
If $K_j = 0$, $B_j$ is set to be zero; otherwise, $B_j$ follows the  $\mbox{N}(0,\sigma^2)$ distribution, where $\sigma$ is a positive hyperparameter.
Thus, $K$ indicates the collection of relevant predictors.
The intercept $A$ is independent of $(B,K)$ and follows the $\mbox{N}(0,\sigma^2)$ distribution.
Let $Z = (A, B^{\top})^{\top}$.
%	Let $Z = (A,\eta_K(B)^{\top})^{\top}$, where $\eta_K$ extracts the elements of an $r$-dimensional vector whose indices are in $\mathsf{J}_K$.
%	Then the model index~$K$ indicates which predictors are relevant, and~$Z$ is a vector of the intercept and the nonzero regression coefficients.
Having observed $Y = (Y_1, \dots, Y_n)^{\top}$, the goal is to make inference about $(K, Z)$.

The parameter space, i.e., the range of $(K,Z)$, is $\X = \bigcup_{k \in \mathsf{K}} \{k\} \times \Z_k$, where $\mathsf{K} = \{0,1\}^r$, and,
for $k = (k_1, \dots,k_r) \in \mathsf{K}$,
%$\Z_k = \mathbb{R}^{|\mathsf{J}_k|+1}$.
$\Z_k$ is the set of $(\alpha, \beta_1, \dots, \beta_r) \in \mathbb{R}^{r+1}$ such that $\beta_j = 0$ whenever $k_j = 0$.
The posterior distribution~$\Pi$ of $(K,Z)$ given $Y = y = (y_1, \dots, y_n)$ is of the form~\eqref{eq:Pi}.
Evaluated at $z = (\alpha,\beta^{\top})^{\top} \in \Z_k$, the density function of $\Psi_k$ is
\[
\begin{aligned}
	\pi(k, z \mid y) =& \frac{1}{\sqrt{2\bm{\pi}} \sigma} \left( \frac{p}{\sqrt{2 \bm{\pi}} \sigma} \right)^{|\mathsf{J}_k|} \exp \left( - \frac{ \alpha^2 + \sum_{j \in \mathsf{J}_k} \beta_j^2 }{2 \sigma^2} \right) \\
	&\prod_{i=1}^n F \left( \alpha + \sum_{j \in \mathsf{J}_k} x_{i,j} \beta_j \right)^{y_i} \left[ 1 - F \left( \alpha + \sum_{j \in \mathsf{J}_k} x_{i,j} \beta_j \right)  \right]^{1-y_i}.
\end{aligned}
\]
%	Evaluated at $z = (\alpha,\eta_k(\beta)^{\top})^{\top}$, where $\alpha \in \mathbb{R}$ and $\beta = (b_1, \dots, b_r)^{\top} \in \mathbb{R}^r$, the density function of $\Psi_k$ is
%	\[
%	\begin{aligned}
	%		\pi(k, z \mid y) =& \frac{1}{\sqrt{2\bm{\pi}} \sigma} \left( \frac{p}{\sqrt{2 \bm{\pi}} \sigma} \right)^{|\mathsf{J}_k|} \exp \left( - \frac{ \alpha^2 + \sum_{j \in \mathsf{J}_k} b_j^2 }{2 \sigma^2} \right) \\
	%		&\prod_{i=1}^n F \left( \alpha + \eta_k(x_i)^{\top} \eta_k(\beta) \right)^{y_i} \left[ 1 - F \left( \alpha + \eta_k(x_i)^{\top} \eta_k(\beta) \right)  \right]^{1-y_i}.
	%	\end{aligned}
%	\]



\subsubsection{A reversible jump MCMC algorithm}

For a fixed model $k=(k_1,\dots,k_r)$, one can use a well-known data augmentation algorithm (a type of reversible MCMC algorithm) devised by \cite{albert1993bayesian} to sample from $\Phi_k$, the normalization of $\Psi_k$.
% See \cite{chakraborty2016convergence} for a concise description of the algorithm.
	The algorithm is now briefly described.
	Suppose that $\mathsf{J}_k = \{j_1, \dots, j_d\}$, where $d = |\mathsf{J}_k|$ and $j_1 < \cdots < j_d$.
	Let $M(k)$ denote the $n \times (d+1)$ matrix whose $i$th row is $(1, x_{i,j_1}, \dots, x_{i,j_d} )^{\top}$.
	Given the current state $z = (\alpha, \beta_1, \dots, \beta_r)^{\top} \in \Z_k$, the next state $z'= (\alpha', \beta'_1, \dots, \beta'_r)^{\top}$ is drawn through the following procedure:
	Independently for $i = 1,\dots,n$, draw $u_i$ from the $\mbox{N}( \alpha + \sum_{j=1}^r x_{i,j} \beta_j , 1^2 )$ distribution truncated to $(0,\infty)$ if $y_i = 1$ and to $(-\infty,0)$ otherwise.
	Let $u = (u_1, \dots, u_n)^{\top}$.
	Draw $(\alpha', \beta'_{j_1}, \dots, \beta'_{j_d})^{\top}$ from the normal distribution
	\[
	\mbox{N}_{d+1} \left( \left[M(k)^{\top}M(k) + \sigma^{-2} I_{d+1} \right]^{-1} M(k)^{\top} u, \; \left[M(k)^{\top} M(k) + \sigma^{-2} I_{d+1} \right]^{-1}  \right),
	\]
	and set the remaining elements of $z'$ to zero.
%	For $k \in \{0,1\}^r$, let $X(k)$ denote the $n \times (|\mathsf{J}_k|+1)$ matrix whose $i$th row is $x_i^*(k)^{\top} = (1,\eta_k(x_i)^{\top})$.
%	Given the current state $z \in \Z_k$, the next state $z'$ is drawn through the following procedure:
%	Independently for $i = 1,\dots,n$, draw $u_i$ from the $\mbox{N}( x_i^*(k)^{\top} z , 1^2 )$ distribution truncated to $(0,\infty)$ if $y_i = 1$ and to $(-\infty,0)$ otherwise.
%	Let $u = (u_1, \dots, u_n)^{\top}$.
%	Draw $z'$ from the normal distribution
%	\[
%	\mbox{N}_{|\mathsf{J}_k|+1} \left( \left[X(k)^{\top}X(k) + \sigma^{-2} I_{|\mathsf{J}_k|+1} \right]^{-1} X(k)^{\top} u, \; \left[X(k)^{\top} X(k) + \sigma^{-2} I_{|\mathsf{J}_k|+1} \right]^{-1} \right).
%	\]



The reversible jump MCMC algorithm considered herein is a combination of the data augmentation algorithm and a standard reversible jump scheme.
Given the current state $(k,z)$, the algorithm randomly performs a U (update), B (birth), or D (death) move.
It is assumed that the probability of choosing a move depends on $(k,z)$ only through~$k$, and the probabilities are denoted by $q_{\scriptsize\mbox{U}}(k)$, $q_{\scriptsize\mbox{B}}(k)$, and $q_{\scriptsize\mbox{D}}(k)$, respectively.

\begin{itemize}
	\item {\it U move}: 
	Draw $z'$ using one iteration of Albert and Chib's data augmentation algorithm.
	Set the new state to $(k,z')$.
	
	\item {\it B move}:
	Randomly and uniformly choose an index $j$ from $\mathsf{J}_k^c$, where the complement is taken with respect to the set $\{1,\dots,r\}$.
	Change the $j$th element of~$k$ to~1 and call the resultant binary vector $k'$.
	Draw $b_*$ from some distribution on~$\mathbb{R}$ associated with a density function $g(\cdot \mid k, k', z, y)$.
	Replace the $(j+1)$th element of $z = (\alpha, \beta_1, \dots, \beta_r)^{\top}$ by $b_*$, and call the resultant vector $z'$.
	%		Let $\alpha \in \mathbb{R}$ and $\beta  \in \mathbb{R}^r$ be such that $(\alpha, \eta_k(\beta)^{\top})^{\top} = z$.
	%		Replace the $j$th element of~$\beta$ by~$b_*$, and call the resultant vector~$\beta'$.
	%		Let $z' = (\alpha, \eta_{k'}(\beta')^{\top})^{\top}$.
	With probability
	\[
	\min \left\{ 1, \frac{ \pi(k', z' \mid y) \, q_{\scriptsize\mbox{D}}(k') \,  (|\mathsf{J}_k|+1)^{-1} }{ \pi(k,z \mid y) \, q_{\scriptsize\mbox{B}}(k) \, (r - |\mathsf{J}_k|)^{-1} g(b_* \mid k, k', z, y)  }  \right\},
	\]
	set the next state to $(k',z')$;
	otherwise, keep the old state.
	This move type is available only when $|\mathsf{J}_k| < r$.
	
	\item {\it D move}:
	Randomly and uniformly choose an index $j$ from $\mathsf{J}_k$.
	Change the $j$th element of~$k$ to 0 and call the resultant binary vector $k'$.
	Let $\beta_j$ be the $(j+1)$th element of $z$.
	Let $z'$ be the vector obtained by changing the $(j+1)$th element of $z$ to 0.
	%		Let $\alpha \in \mathbb{R}$ and $\beta  \in \mathbb{R}^r$ be such that $(\alpha, \eta_k(\beta)^{\top})^{\top} = z$, and set $z' = (\alpha, \eta_{k'}(\beta)^{\top})^{\top}$.
	%		Note that~$z'$ is just~$z$ with an element deleted; call the deleted element~$b_*$.
	With probability
	\[
	\min \left\{ 1, \frac{\pi(k',z' \mid y) \, q_{\scriptsize\mbox{B}}(k') \, (r - |\mathsf{J}_k| + 1)^{-1} g(\beta_j \mid k', k, z', y)}{\pi(k, z \mid y) \, q_{\scriptsize\mbox{D}}(k) \, |\mathsf{J}_k|^{-1}} \right\},
	\]
	set the next state to $(k',z')$;
	otherwise, keep the old state.
	This move type is available only when $|\mathsf{J}_k| > 0$.
\end{itemize}

The resultant trans-dimensional chain is reversible with respect to~$\Pi$.


\subsubsection{Convergence analysis}



For $k \in \mathsf{K}$, let $P_k$ be the Mtk of the data augmentation chain targeting $\Phi_k$.
\cite{chakraborty2016convergence} proved the following result regarding $P_k$ using the drift and minorization technique.
See also \cite{roy2007convergence}.

\begin{lemma} \label{lem:acgeo}
	For $k \in \mathsf{K}$, $P_k$ is $\Phi_k$-a.e. geometrically ergodic.
\end{lemma}

Given~$k$, the data augmentation Mtk $P_k$ is reversible with respect to $\Phi_k$.
One can then establish geometric convergence for the reversible jump chain.

\begin{proposition} \label{pro:acgeo}
	Suppose that $q_{\scriptsize\mbox{U}}(k) > 0$ for $k \in \mathsf{K}$, $q_{\scriptsize\mbox{B}}(k) > 0$ when $|\mathsf{J}_k| < r$, $q_{\scriptsize\mbox{D}}(k) > 0$ when $|\mathsf{J}_k| > 0$.
	Then the reversible jump chain is $L^2(\Pi)$-geometrically convergent and $\Pi$-a.e. geometrically ergodic.
\end{proposition}

\begin{proof}
	Apply Theorem~\ref{thm:main}.
	Let~$P$ be the Mtk of the reversible jump chain.
	By Lemma~\ref{lem:acgeo} and (i) of Lemma~\ref{lem:Pk-convergence}, $\|P_k\|_{\Phi_k} < 1$ for $k \in \mathsf{K}$.
	It follows that, for $k \in \mathsf{K}$, (i) and (ii) of \ref{H1} hold with $t_0 = 1$.
	Evidently, (iii) of \ref{H1} also holds with $c_k = q_{\scriptsize\mbox{U}}(k)$.
	
	To verify \ref{H2}, note that for $k,k' \in \mathsf{K}$, $\bar{P}(k,\{k'\}) > 0$ whenever $k$ and $k'$ differ by at most 1 element.
	Then it is clear that $\bar{P}$ is irreducible.
	(Alternatively, note that $P$ is $\Pi$-irreducible.)
	
	
	The chain is thus $L^2(\Pi)$ geometrically convergent.
	By Theorem 1 of \cite{roberts2001geometric}, it is $\Pi$-a.e. geometrically ergodic.
\end{proof}

\subsubsection{Application to a data set} \label{sssec:binary-example}



Geometric ergodicity allows one to estimate the importance of features in a variable selection problem with confidence.
The reversible jump algorithm is applied to the \verb|Spambase| data set \citep{misc_spambase_94}.
This data set contains $n = 4601$ emails. 
The response $Y_i$ indicates whether the $i$th email is spam.
Each email is associated with $r = 57$ attributes, including the frequency of certain words and the length of sequences of consecutive capital letters.
To perform variable selection, a spike and slab prior with $p = 0.5$ is used.

In the B move of the reversible jump algorithm, $g(\cdot \mid k, k', z, y)$ is chosen to be the density of a normal distribution, whose parameters are selected using ideas from \cite{brooks2003efficient}.
%In the B move of the reversible jump algorithm, $g(\cdot \mid k, k', z, y)$ is chosen to be the density of a normal distribution, whose parameters are specified below.
%Recall that $k'$ and~$k$ differ by exactly one element, whose index is~$j$.
%For $b \in \mathbb{R}$, let $z_{b}$ be the vector obtained by changing the $(j+1)$th element of $z$ to $b$.
%Then $b \mapsto \pi(k', z_{b} \mid y)$ is a log-concave function.
%The mean of the normal distribution is the maximizer of this function, while the variance is the negative second-order derivative at the maximizer.
%The construction uses ideas from \cite{brooks2003efficient}.
The probabilities of proposing birth and death moves are as follows:
\[
q_{\scriptsize\mbox{B}}(k) = \frac{1}{3} \min \left\{ 1, \frac{p \, (r - |\mathsf{J}_k|)}{|\mathsf{J}_k| + 1} \right\}, \quad q_{\scriptsize\mbox{D}}(k) = \frac{1}{3} \min \left\{ 1, \frac{|\mathsf{J}_k|}{p \, (r - |\mathsf{J}_k| + 1)} \right\}.
\]
%	Under this construction, in the B move,
%	\[
%	\frac{f_K(k') q_{\scriptsize\mbox{D}}(k') (|\mathsf{J}_k|+1)^{-1}}{f_K(k) q_{\scriptsize\mbox{B}}(k) (r - |\mathsf{J}_k|)^{-1} } = 1.
%	\]
By Proposition~\ref{pro:acgeo}, the reversible jump chain is $\Pi$-a.e. geometrically ergodic.

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{probit.subset}
	\caption{Estimated posterior probabilities of selecting each variable (attribute), with 95\% simultaneous confidence intervals.
		Only the last 10 variables are shown.
	} \label{fig:probit}
\end{figure}

A chain $(K(t), Z(t))_{t=1}^m$ of length $m = 10^5$ is simulated.
The quantities of interest are the posterior probabilities of $K_j = 1$ for $j = 1,\dots,57$, i.e., the posterior probability of any given predictor being present in the regression model.
They are estimated using the sample proportions $m^{-1} \sum_{t=1}^m K_j(t)$.
Under geometric ergodicity, the sample proportions are asymptotically normally distributed, and the asymptotic variances can be consistently estimated using the batch means method \citep{jones2006fixed}.
To avoid underestimation, which is a problem exhibited by batch means estimators when the Monte Carlo sample size is not sufficiently large \citep[][Section 4]{flegal2010batch}, we add $(\log m) \sqrt{1/b_m^2+b_m/m}$ to the estimated asymptotic variances, where $b_m \approx m^{0.6}$ is the batch size, and $1/b_m^2 + b_m/m$ is on the same order as the mean squared error of the batch means estimator \citep[][Section 3]{flegal2010batch}.
This adjustment is further discussed in Section \ref{app:wald} of \ref{s1}.
We construct 95\% simultaneous Wald confidence intervals for the posterior probabilities.
Bonferroni correction is used here, although more sophisticated multivariate methods could be considered; see Section \ref{sec:discussion}.
The confidence intervals for the last 10 variables (attributes) are presented in Figure~\ref{fig:probit}.
This figure shows how important each predictor is according to the MCMC simulation, as well as the errors of their estimated importance.





{\subsection{Gaussian mixture model} \label{ssec:mixture}}

\subsubsection{The model}

%Trans-dimensional MCMC naturally applies to mixture models \citep{richardson1997bayesian,zhang2004learning}.
%Consider the following Gaussian mixture model.

Let $Y_1, \dots, Y_n$ be iid random variables drawn from the mixture of $K$ normal distributions.
For $j = 1,\dots,K$, let $W_j$ be the weight associated with the $j$th normal distribution, and let $U_j$ and $T_j$ be, respectively, the mean and variance of that normal distribution.
Equivalently, we may formulate the model as follows.
Let $(Y_1, A_1), \dots, (Y_n, A_n)$ be iid random vectors that take values in $\mathbb{R} \times \{1,\dots,k\}$, where, for each $i$, $Y_i$ given $A_i$ follows the $\mbox{N}(U_{A_i}, T_{A_i})$ distribution, and marginally $P(A_i = j) = W_j$ for $j = 1,\dots,K$.
Suppose that $Y = (Y_1, \dots, Y_n)$ is observable, while $A = (A_1, \dots, A_n)$, $W = (W_1, \dots, W_K)$, $U = (U_1, \dots, U_K)$, $T = (T_1, \dots, T_K)$, and~$K$ are unknown.

To perform Bayesian analysis, we assume that $K$ has a prior probability mass function $k \mapsto f_K(k)$ that is supported on a finite set $\mathsf{K} = \{1, \dots, k_{\scriptsize\max}\}$.
We then put a Dirichlet prior on $W$, inverse gamma priors on $T$, and normal priors on $U$.
To address the label switching problem and enforce identifiability, we shall assume that $U_1 \leq U_2 \leq \cdots \leq U_K$ in the prior distribution.
To be precise, we shall assume that, given $K = k$, the prior density function of $(W,T,U)$ evaluated at $(w,\tau,u) = ((w_1, \dots, w_k), (\tau_1, \dots, \tau_k), (u_1, \dots, u_k))$ has the following form:
\begin{equation} \label{eq:fWTU}
	\begin{aligned}
		f_{W,T,U}(w,\tau, u \mid k) =& \left\{ \prod_{j=1}^k w_j^{\gamma - 1} \frac{b^c}{\Gamma(c)} \tau_j^{-c - 1} e^{-b/\tau_j} \frac{1}{\sqrt{2 \bm{\pi} \tau_0 \tau_j}} \exp\left[ - \frac{(u_j-u_0)^2}{2\tau_0 \tau_j}  \right] \ind_{(0,\infty)}(\tau_j) \right\} \\
		&  \frac{\Gamma(k\gamma)}{\Gamma(\gamma)^k} \ind_{\mathsf{S}_k}(w) \, k! \, \ind_{\mathsf{G}_k}(u).
	\end{aligned}
\end{equation}
In the above display, $\gamma, b, c, \tau_0, u_0$ are positive hyperparameters, $\Gamma(\cdot)$ is the gamma function, $\mathsf{S}_k = \{(w_1, \dots, w_k) \in (0,1)^k: \, \sum_{j=1}^k w_j = 1 \}$, $\mathsf{G}_k = \{(u_1, \dots, u_k) \in \mathbb{R}^k: \, u_1 \leq \cdots \leq u_k\}$.

The un-normalized posterior density of $(K,A,W,T,U)$ is then
\[
\pi(k,\alpha,w,\tau,u \mid y) = \left\{ \prod_{i=1}^n w_{\alpha_i} \frac{1}{\sqrt{2\bm{\pi} \tau_{\alpha_i} } } \exp\left[ - \frac{1}{2 \tau_{\alpha_i}} (y_i - u_{\alpha_i})^2 \right] \right\} f_{W,T,U}(w,\tau,u \mid k) f_K(k),
\]
where $\alpha_i$ denotes the $i$th element of $\alpha$.
The corresponding measure $\Pi$ has the form \eqref{eq:Pi}, with the density of $\Psi_k$ given by $(\alpha,w,\tau,u) \mapsto \pi(k, \alpha,w,\tau,u \mid y)$.
In this context, $\Z_k = \{1,\dots,k\}^n \times \mathsf{S}_k \times (0,\infty)^k \times \mathsf{G}_k$.

\subsubsection{A reversible jump algorithm}

The algorithm we consider is modified from \pcite{richardson1997bayesian} reversible jump algorithm, with a new within-model move type and simplified between-model move types.

We shall first propose an algorithm for sampling from $\Phi_K$ when $K$ is known.
For $k \in \mathsf{K}$ and $(w,\tau,u) \in \mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k$, let $\tilde{f}_{W,T,U}(w,\tau,u \mid k)$ be the same as \eqref{eq:fWTU} but without the constraint $\ind_{\mathsf{G}_k}(u)$.
It can be shown that, given $k \in \mathsf{K}$, for $\alpha \in \{1,\dots,k\}^n$,
\[
\begin{aligned}
	&\tilde{\pi}_A(\alpha \mid k, y) \\
	:=& \int_{\mathsf{S}_k \times (0,\infty)^p \times \mathbb{R}^p} \left\{ \prod_{i=1}^n w_{\alpha_i} \frac{1}{\sqrt{2\bm{\pi} \tau_{\alpha_i} } } \exp\left[ - \frac{1}{2 \tau_{\alpha_i}} (y_i - u_{\alpha_i})^2 \right] \right\}  \tilde{f}_{W,T,U}(w,\tau,u) \, \df (w, \tau, \mu) \\
	\propto & \prod_{j=1}^k  \frac{ [\tau_0 \, n_j(\alpha) + 1]^{-1/2} \, \Gamma(n_j(\alpha) + \gamma) \, \Gamma(n_j(\alpha)/2 + c) }{ \left\{ \mathit{ss}_j(\alpha)/2 +u_0^2/(2\tau_0) -[ s_j(\alpha) + u_0/\tau_0]^2/[2 (n_j(\alpha) + 1/\tau_0)] + b \right\}^{n_j(\alpha)/2 + c} },
\end{aligned}
\]
where $n_j(\alpha) = \sum_{i=1}^n \ind_{\{j\}}(\alpha_i)$, $s_j(\alpha) = \sum_{i=1}^n  y_i \ind_{\{j\}}(\alpha_i)$, and $\mathit{ss}_j(\alpha) = \sum_{i=1}^n y_i^2 \ind_{\{j\}}(\alpha_i)$.
Let $z = (\alpha,w,\tau,u) \in \Z_k$ be the current state.
The next state $z' = (\alpha',w',\tau',u')$ is drawn via the following steps:
\begin{enumerate}
	\item Let $\alpha^{(0)} = (\sigma(\alpha_1), \dots, \sigma(\alpha_n))$, where $\sigma$ is a randomly and uniformly selected permutation of $\{1,\dots,k\}$.
	\item 
	For $i$ from 1 to $n$, do the following.
	Randomly and uniformly draw $j_i$ from $\{1,\dots,k\}$.
	Let $\alpha^{(i-1)}_{i \leftarrow j_i} \in \{1,\dots,k\}^n$ be the vector obtained from $\alpha^{(i-1)}$ by changing its~$i$th element to~$j_i$.
	With probability
	\[
	\min \left\{ 1, \frac{\tilde{\pi}_A(\alpha^{(i-1)}_{i \leftarrow j_i} \mid k, y)}{\tilde{\pi}_A(\alpha^{(i-1)} \mid k, y)} \right\},
	\]
	let $\alpha^{(i)} = \alpha^{(i-1)}_{i \leftarrow j_i}$; otherwise, let $\alpha^{(i)} = \alpha^{(i-1)}$.
	Denote $\alpha^{(n)}$ by $\alpha''$.
	
	\item 
	Draw $w'' = (w''_1, \dots, w''_k)$ from the Dirichlet distribution with concentration parameter $(n_1(\alpha'') + \gamma, \dots, n_k(\alpha'') + \gamma)$.
	
	\item 
	For $j = 1,\dots, k$, independently draw $\tau''_j$ from the inverse gamma distribution with shape parameter $c + n_j(\alpha'')/2$ and scale parameter
	\[
	\frac{\mathit{ss}_j(\alpha'')}{2} + \frac{u_0^2}{2\tau_0} - \frac{[s_j(\alpha'')+\mu_0/\tau_0]^2}{2[ n_j(\alpha'') + 1/\tau_0]} + b.
	\]
	
	\item For $j = 1,\dots,k$, independently draw $u_1'', \dots, u_k''$ from the normal distribution
	\[
	\mbox{N} \left( \frac{s_j(\alpha'') + u_0/\tau_0}{n_j(\alpha'') + 1/\tau_0}, \, \frac{\tau_j''}{n_j(\alpha'') + 1/\tau_0} \right).
	\]
	
	\item 
	Order $u_1'', \dots, u_j''$ so that we find distinct indices $j_1, \dots, j_k$ such that $u_{j_1}'' \leq \cdots \leq u_{j_k}''$.
	For $i = 1,\dots,n$, find the index $\ell_i \in \{1,\dots,k\}$ such that $\alpha_i'' = j_{\ell_i}$, and let $\alpha_i' = \ell_i$.
	For $\ell = 1,\dots,k$, let $w_{\ell}' = w_{j_{\ell}}''$, $\tau_{\ell}' = \tau_{j_{\ell}}''$, and $u_{\ell}' = u_{j_{\ell}}''$.
\end{enumerate}

We shall call the sampler the Metropolis re-ordering algorithm.
The following lemma is proved in Section~\ref{app:reordering-invariant} of \ref{s1}.
\begin{lemma} \label{lem:reordering}
	The underlying Markov chain of the Metropolis re-ordering algorithm leaves $\Phi_k$ invariant for $k \in \mathsf{K}$.
\end{lemma}
A nice property of the Metropolis re-ordering algorithm is that starting from any allocation $\alpha \in \{1,\dots,k\}^n$, it is possible for the chain to move to any allocation $\alpha' \in \{1,\dots,k\}^n$ in a single iteration.

Consider now a reversible jump algorithm for sampling from~$\Pi$, which is part of an algorithm constructed by \cite{richardson1997bayesian}.
When the current state is $(k,z) = (k,\alpha,w,\tau,u)$, the algorithm randomly performs a U, B, or D move.
It is assumed that the probabilities of choosing these moves depend on the current state only through~$k$, and are, respectively, $q_{\scriptsize\mbox{U}}(k)$, $q_{\scriptsize\mbox{B}}(k)$, $q_{\scriptsize\mbox{D}}(k)$.
The three move types are defined as follows:

\begin{itemize}
	\item {\it U move}:
	Draw $z'$ using one iteration of the Metropolis re-ordering algorithm.
	Set the new state to $(k,z')$.
	
	\item {\it B move}:
	Draw $(w_*, \tau_*, u_*)$ from some distribution on $(0,1) \times (0,\infty) \times \mathbb{R}$ with density function $g(\cdot \mid k, \alpha, w, \tau, u)$.
	%		Draw $w_*$ from the beta distribution with shape parameters~1 and~$n$ (so that its mean is $1/(1+n)$).
	%		Draw $\tau_*$ from the inverse gamma distribution with shape parameter $c$ and scale parameter $b$.
	%		Draw $u_*$ from the normal distribution with mean~0 and variance $\tau_0 \tau_*$.
	Find $\ell \in \{1,\dots,k+1\}$ such that $u_j \leq u_*$ whenever $j < \ell$, and $u_j \geq u_*$ whenever $j \geq \ell$ --- that is, $u_*$ is the $\ell$th smallest number in the set $\{u_1, \dots, u_k, u_*\}$.
	Let $\alpha' = (\alpha_1', \dots, \alpha_n')$ be such that, for $i \in \{1,\dots,n\}$, $\alpha_i' = \alpha_i$ if $\alpha_i < \ell$ and $\alpha_i' = \alpha_i + 1$ if $\alpha_i \geq \ell$.
	Let
	\[
	\begin{aligned}
		&w' = ((1-w_*)w_1, \dots, (1-w_*) w_{\ell-1}, w_*, (1-w_*) w_{\ell}, \dots, (1-w_*) w_k ), \\
		&\tau' = (\tau_1, \dots, \tau_{\ell-1}, \tau_*, \tau_{\ell}, \dots, \tau_k), \quad u' = (u_1, \dots, u_{\ell-1}, u_*, u_{\ell}, \dots, u_k).
	\end{aligned}
	\]
	With probability
	\[
	\min\left\{ 1, \frac{\pi(k+1, \alpha', w', \tau', u' \mid y) \, q_{\scriptsize\mbox{D}}(k+1) \, [\sum_{j=1}^{k+1} \ind_{\{0\}}(n_j(\alpha')) ]^{-1} \, (1-w_*)^{k-1} }{ \pi(k, \alpha, w, \tau, u \mid y) \, q_{\scriptsize\mbox{B}}(k) \, g(w_*, \tau_*, u_* \mid k, \alpha, w, \tau, u) }  \right\},
	\]
	set the new state to $(k+1, \alpha', w', \tau', u')$;
	otherwise, keep the old state.
	This move is available only when $k < k_{\scriptsize\mbox{max}}$.
	
	\item {\it D move}:
	Let $\mathsf{E}_k(\alpha) \subset \{1,\dots,k\}$ be the set of $j$'s such that $n_j(\alpha) = 0$.
	Keep the old state if $\mathsf{E}_k(\alpha) = \emptyset$, and follow the procedure below otherwise.
	Randomly and uniformly select $\ell$ from $\mathsf{E}_k(\alpha)$.
	Let $\alpha' = (\alpha'_1, \dots, \alpha'_n)$ be such that, for $i = 1,\dots,n$, $\alpha'_i = \alpha_i$ if $\alpha_i < \ell$ and $\alpha'_i = \alpha_i - 1$ if $\alpha_i > \ell$.
	Let
	\[
	\begin{aligned}
		&w' = (w_1/(1-w_{\ell}), \dots, w_{\ell-1}/(1-w_{\ell}), w_{\ell+1}/(1-w_{\ell}), w_k/(1-w_{\ell})), \\
		&\tau' = (\tau_1, \dots, \tau_{\ell-1}, \tau_{\ell+1}, \dots, \tau_k), \quad u' = (u_1, \dots, u_{\ell-1}, u_{\ell+1}, \dots, u_k).
	\end{aligned}
	\]
	With probability
	\[
	\min \left\{ 1, \frac{\pi(k-1, \alpha', w', \tau', u' \mid y) \, q_{\scriptsize\mbox{B}}(k-1) \, g(w_{\ell}, \tau_{\ell}, u_{\ell} \mid k-1, \alpha', w', \tau', u') }{ \pi(k, \alpha, w, \tau, u \mid y) \,  q_{\scriptsize\mbox{D}}(k) \, [\sum_{j=1}^k \ind_{\{0\}}(n_j(\alpha)) ]^{-1} \, (1-w_{\ell})^{k-2}}  \right\},
	\]
	set the new state to $(k-1, \alpha', w', \tau', u')$; otherwise, keep the old state.
	This move is available only when $k > 1$.
\end{itemize}



\begin{figure} 
	\begin{center}
		\includegraphics[width=0.48\textwidth]{mixturek} \; \includegraphics[width=0.48\textwidth]{mixtureu} \\
		\includegraphics[width=0.48\textwidth]{mixturehistogram} \;
		\includegraphics[width=0.48\textwidth]{mixture.intervals}
		\caption{Top left: trace plot of $K(t)$; top right: trace plot of $U_2(t)$ when $K(t) = 5$;
			bottom left: histogram and predictive density for the galaxy data set; 
			bottom right: estimated posterior probabilities of $K = k$ and their 95\% simultaneous confidence intervals.
			Hyperparameters: $k_{\scriptsize\mbox{max}} = 30$, $\gamma = 2$, $b = 2$, $c = 3$, $\tau_0 = 1000$, $u_0 = 20$.
			$f_K(k) \propto 1/2^k$. 
		} \label{fig:galaxy}
	\end{center}
\end{figure}


The resultant chain has $\Pi$ as its stationary distribution due to the reversible jump construction.
On the other hand, the Metropolis-reordering algorithm is not classified as a well-known reversible algorithm, and it remains unclear whether the U move type is reversible or positive semi-definite.


For illustration, the algorithm is applied to the galaxy data set described by \cite{roeder1990density} and studied by \cite{richardson1997bayesian}.
In the B move, $w_*$ is drawn from the beta distribution with parameters $1$ and $n$, $\tau_*$ is drawn from the inverse gamma distribution with shape parameter~$c$ and scale parameter~$b$, and $u_*$ is drawn from the $\mbox{N}(0, \tau_0 \tau_*)$ distribution.
Following \cite{green1995reversible}, the birth and death probabilities are set to
\[
q_{\scriptsize\mbox{B}}(k) = \frac{1}{3} \min \left\{ 1, \frac{f_K(k+1)}{f_K(k)} \right\}, \, q_{\scriptsize\mbox{D}}(k) = \frac{1}{3} \min \left\{ 1, \frac{f_K(k-1)}{f_K(k)} \right\}.
\]
The trans-dimensional chain $(K(t), A(t), W(t), T(t), U(t))_{t=1}^{\infty}$ is simulated for $10^5$ iterations.
The empirical performance of the algorithm is shown in Figure \ref{fig:galaxy}.
The predictive density evaluated at a point~$x$ is the sample average of $\sum_{j=1}^{K(t)} W_j(t) f(x \mid U_j(t), T_j(t))$, where $f(\cdot \mid u, \tau)$ denotes the density of the $\mbox{N}(u,\tau)$ distribution.
We can see that the sampler appears to perform well empirically, especially in terms of within-model moves.


\subsubsection{Convergence analysis}

For $k \in \mathsf{K}$, let $P_k$ be the Mtk associated with the Metropolis re-ordering algorithm targeting $\Phi_k$.
We prove the following lemma in Section \ref{app:reordering-geo} of \ref{s1}.
The proof is constructed based on the fact that, in the Metropolis re-ordering algorithm, the next state depends on the current state $(\alpha,w,\tau,u)$ only through $\alpha$, which takes value in a finite set.

{
\begin{lemma} \label{lem:reordering-geo}
	For $k \in \mathsf{K}$, $\|P_k^2\|_{\Phi_k} < 1$.
\end{lemma}
}

With Lemma \ref{lem:reordering-geo} in hand, it is now straightforward to establish the geometric ergodicity of the reversible jump algorithm.

\begin{proposition}
	Suppose that $q_U(k) > 0$ for $k \in \mathsf{K}$, $q_B(k) > 0$ for $k < k_{\scriptsize\mbox{max}}$, and $q_D(k) > 0$ for $k > 0$.
	Then the reversible jump chain is $L^2(\Pi)$ geometrically convergent and $\Pi$-a.e. geometrically ergodic.
\end{proposition}

\begin{proof}
	Apply Theorem \ref{thm:main}.
	Let $P$ be the Mtk of the reversible jump chain.
	For $k \in \mathsf{K}$, (i) and (iii) in \ref{H1} hold with $c_k = q_U(k)$.
	By Lemma \ref{lem:reordering-geo}, (ii) in \ref{H1} holds with $t_0 = 2$.
	
	To verify \ref{H2}, note that $\bar{P}(k,\{k'\}) > 0$ whenever $|k-k'| \leq 1$.
	It is then evident that $\bar{P}$ is irreducible.
	
	Thus, $P$ is $L^2(\Pi)$ geometrically convergent.
	By Theorem~1 of \cite{roberts2001geometric}, it is $\Pi$-a.e. geometrically ergodic.
\end{proof}

\begin{remark}
	The reversible jump algorithm can be further improved by adding more sophisticated between-model move types such as split and merge \citep{richardson1997bayesian,zhang2004learning}.
	Geometric convergence is preserved as long as the between-model movements remain irreducible.
\end{remark}

Geometric ergodicity allows us to construct 95\% simultaneous Wald confidence intervals for the posterior probabilities of $K = k$ for $k = 1,\dots, k_{\scriptsize\mbox{max}}$.
This is shown for the galaxy data set in Figure \ref{fig:galaxy}.
We have added $(\log m) \sqrt{1/b_m^2+b_m/m}$ to the estimated asymptotic variances of the estimated probabilities, as in Section \ref{sssec:binary-example}.

{\section{Discussion} \label{sec:discussion} }

$L^2(\Pi)$ geoemtrically convergence implies $\Pi$-a.e. geometric ergodicity.
Harris recurrence, a regularity condition commonly used in MCMC analysis, can be enforced provided that we restrict our attention to some absorbing set \citep[][Theorem 9.0.1]{meyn2012markov}.
See also \cite{roberts2006harris} for conditions for Harris ergodicity in the context of trans-dimensional chains.
%We provide an extension of \pcite{meyn2012markov} Theorem 9.0.1 in Section \ref{app:ae} of \ref{s1}, which holds under simpler conditions.

Under geometric ergodicity, one may utilize methods from e.g., \cite{jones2006fixed,vats2019multivariate} to construct confidence regions for uncertainty quantification.
In our examples, we used Bonferroni correction to construct multiple Wald confidence intervals, but more sophisticated methods exist \citep{vats2019multivariate,robertson2020assessing}.
% In Section \ref{app:robertson} of \ref{s1}, we provide a theoretical validation of \pcite{robertson2020assessing} technique for constructing multiple confidence intervals.

{
Some of the formulas in the proof of Lemma \ref{lem:decomposition} (found in Section \ref{app:decomposition} of \ref{s1}) can be formulated in terms of Dirichlet forms.
Dirichlet forms may be used to study Markov chains in terms of the conductance \citep{lawler1988bounds}, Peskun-Tierney ordering \citep{andrieu2019peskun}, and functional inequalities \citep{power2024weak}.
}

An obvious avenue for future research is obtaining practical quantitative convergence bounds for trans-dimensional chains.
In particular, ways of computing or estimating $\mbox{Gap}_{\bar{\Pi}}(\overline{P^*P})$ and $\mbox{Gap}_{\bar{\Pi}}(\bar{P})$ would be useful for selecting good proposal distributions in a reversible jump algorithm. 
Whether our convergence bounds can be further sharpened and extended to the case where $\mathsf{K}$ is infinite is also of interest.


\vspace{0.5cm}

\noindent{\bf Acknowledgment:}
We thank the Editor, an anonymous Associate Editor, and two anonymous Referees for their helpful comments.








\bigskip
\begin{center}
{\large\bf SUPPLEMENTARY MATERIALS}
\end{center}

{\bf \namedlabel{s1}{Supplement I}: Minor Results and Technical Proofs. } This document contains a proof for the assertion that $\Pi$-irreducibility implies \ref{H2}, proofs for Lemmas \ref{lem:decomposition}, \ref{lem:reordering}, \ref{lem:reordering-geo}, and simulation results concerning Monte Carlo confidence intervals.

{\bf \namedlabel{s2}{Supplement II}: Autoregression with Laplace errors. } This document contains an application of the theory and methods herein to an autoregressive model with Laplace errors and unknown model order. 



\bibliographystyle{Chicago}

\bibliography{qinbib}

\end{document}
