\documentclass[12pt]{article}
\usepackage{amsbsy,amsmath,amsthm,amssymb,subcaption}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{hyperref}
\usepackage{bm}
\usepackage{xr}
\externaldocument{RJMCMC-r2}
\usepackage{alphalph}
\usepackage{xcolor}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

\makeatletter
\def\namedlabel#1#2{\begingroup
	#2%
	\def\@currentlabel{#2}%
	\phantomsection\label{#1}\endgroup
}
\makeatother

\newcommand{\df}{\mathrm{d}}
\newcommand{\X}{\mathsf{X}}
\newcommand{\Y}{\mathsf{Y}}
\newcommand{\Z}{\mathsf{Z}}
\newcommand{\SF}{\mathcal{A}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Par}{\X}
\newcommand{\Q}{\hat{Q}}
\newcommand{\prior}{m^{\scriptsize\mbox{pr}}}
\newcommand{\Prior}{M^{\scriptsize\mbox{pr}}}
\newcommand{\pprior}{p^{\scriptsize\mbox{pr}}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\Mtk}{\mtkfont{T}}
\newcommand{\mtkfont}{\mathcal}
\newcommand{\pcite}[1]{\citeauthor{#1}'s \citeyearpar{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%




\begin{document}
	
	%\bibliographystyle{natbib}
	
	\def\spacingset#1{\renewcommand{\baselinestretch}%
		{#1}\small\normalsize} \spacingset{1}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\if1\blind
	{
		\title{\bf Supplement I: Minor Results and Technical Proofs}
		\author{}
		\maketitle
	} \fi
	
	\if0\blind
	{
		\title{\bf Supplement I: Minor Results and Technical Proofs}
		\author{Qian Qin\thanks{
				Partially supported by NSF DMS-2112887}\hspace{.2cm}\\
			School of Statistics, University of Minnesota}
		\maketitle
	} \fi

	\maketitle
	
	\spacingset{1.9} % DON'T change the spacing!
	
	\appendix
	
	\renewcommand*{\thetheorem}{\Alph{theorem}}
	\def\theequation{\AlphAlph{\value{equation}}}
	\renewcommand{\thetable}{\Alph{table}}
	
	
	\section{$\Pi$-irreducibility implies \ref{H2}} \label{app:h2}
	
%	Recall that
%	\[
%	\bar{P}(k,\{k'\}) = \int_{\Z_k} \Phi_k(\df z) P((k,z) , \{k'\} \times \Z_{k'}).
%	\]
Assume that $P$ is $\Pi$-irreducible.
Fix $k, k' \in \mathsf{K}$.
By Proposition 4.2 of \cite{meyn2012markov}, for $z \in \Z_k$,
\[
\sum_{t=1}^{\infty} 2^{-t} P^t((k,z), \{k'\} \times \Z_{k'}) > 0.
\]
By the monotone convergence theorem, 
\[
\sum_{t=1}^{\infty} 2^{-t} \int_{\Z_k} \Phi_k(\df z) P^t((k,z), \{k'\} \times \Z_{k'}) > 0.
\]
That is, one can find a positive integer $s$ such that 
\[
\int_{\Z_k} \Phi_k(\df z) P^s((k,z), \{k'\} \times \Z_{k'}) > 0.
\]
Let $\{K(t), Z(t)\}_{t=0}^{\infty}$ be a chain associated with~$P$ such that $K(0) = k$ and $Z(t) \sim \Phi_k$.
Then the above display reads $\mbox{Pr}(K(s) = k') > 0$.
We may partition the event $[K(s) = k']$ according to the path of $K(t)$ for $t = 0,\dots,s$.
It follows that there exist $k(0), \dots, k(s) \in \mathsf{K}$ such that $k(0) = k$, $k(s) = k'$, and that
\[
\mbox{Pr} \left( K(t) = k(t) \text{ for } t = 0,\dots,s \right) > 0.
\] 
In particular, for $t \in \{0,\dots,s-1\}$, $\mbox{Pr}(K(t) = k(t), \, K(t+1) = k(t+1)) > 0$, i.e.,
\[
\int_{\Z_k} \mu_t(\df z) P((k(t),z), \{k(t+1)\} \times \Z_{k(t+1)} ) > 0,
\]
where $\mu_t(\mathsf{A}) = \int_{\Z_k} \Phi_k(\df z) P^t((k,z), \{k(t)\} \times \mathsf{A})$ for $\mathsf{A} \in \A_{k(t)}$.
For $\mathsf{A} \in \A_{k(t)}$,
\[
\mu_t(\mathsf{A}) \leq \frac{1}{\Psi_k(\Z_k)} \sum_{k'' \in \mathsf{K}} \Psi_{k''}(\df z) P^t((k'',z), \{k(t)\} \times \mathsf{A}) = \frac{\Psi_{k(t)}(\mathsf{A})}{\Psi_k(\Z_k)}.
\]
Then it holds that, for $t \in \{0,\dots,s-1\}$,
\[
\int_{\Z_{k(t)}} \Psi_{k(t)}(\df z) \, P((k(t),z), \{k(t+1)\} \times \Z_{k(t+1)} ) > 0,
\]
i.e., $\bar{P}(k(t), \{k(t+1)\}) > 0$.
This in turn implies that $\bar{P}^s(k,\{k'\}) > 0$.
Thus, $\bar{P}$ is irreducible, i.e., \ref{H2} holds.


\section{Proof of Lemma~\ref{lem:decomposition}} \label{app:decomposition}

Recall some notations: 
$(\Y,\F,\omega)$ is a probability space.
$\mtkfont{S}$ and $\Mtk$ are Mtks that have $\omega$ as their stationary distributions.
The state space~$\Y$ has a disjoint decomposition $\Y = \bigcup_{k \in \mathsf{K}} \Y_k$.
For $k \in \mathsf{K}$, $\F_k$ is the restriction of~$\F$ to $\Y_k$, $\omega_k$ is the normalized restriction of~$\omega$ on $\Y_k$, and $\bar{\omega}(\{k\}) = \omega(\Y_k)$.
The Mtk $\Mtk_k: \Y_k \times \F_k \to [0,1]$ satisfies $\omega_k \Mtk_k = \omega_k$, and $\Mtk(y,\mathsf{B}) \geq c\Mtk_k(y, \mathsf{B})$ for $y \in \Y_k$ and $\mathsf{B} \in \F_k$, where $c \in [0,1]$.
Finally, the kernel
\[
\overline{\mtkfont{S}^* \mtkfont{S}}(k,\{k'\}) = \frac{1}{\omega(\Y_k)} \int_{\Y} \omega(\df y) \mtkfont{S}(y, \Y_k) \mtkfont{S}(y, \Y_{k'}).
\]
defines a positive semi-definite operator on $L_0^2(\bar{\omega})$.
The goal is to show that
\[
1 - \|\Mtk \mtkfont{S}^*\|_{\omega}^2 \geq c^2 \left(1 - \sup_{k \in \mathsf{K}} \|\Mtk_k\|_{\omega_k}^2 \right) (1 - \|\overline{\mtkfont{S}^* \mtkfont{S}}\|_{\bar{\omega}} ).
\]




\begin{proof}
	Let $f \in L_0^2(\omega)$ be arbitrary.
	Then
	\begin{equation} \label{eq:fnormdecomp-1}
		\|f\|_{\omega}^2 - \|\Mtk \mtkfont{S}^* f\|_{\omega}^2 = \|f\|_{\omega}^2 - \|\mtkfont{S}^*f\|_{\omega}^2 + \|\mtkfont{S}^*f\|_{\omega}^2 - \|\Mtk \mtkfont{S}^* f\|_{\omega}^2.
	\end{equation}
	For $y \in \Y$ and $\mathsf{B} \in \F$, let
	\[
	\hat{\Mtk}(y,\mathsf{B}) = \sum_{k \in \mathsf{K}} 1_{\Y_k}(y) \Mtk_k(y, \mathsf{B} \cap \Y_k).
	\]
	Then $\omega \hat{\Mtk} = \omega$, and $\Mtk(y, \cdot) \geq c \hat{\Mtk}(y, \cdot)$ for $y \in \Y$.
	Thus, there exists an Mtk $\mtkfont{R}: \Y \times \F \to [0,1]$ such that $\omega \mtkfont{R} = \omega$ and $\Mtk = c\hat{\Mtk} + (1-c) \mtkfont{R}$.
	{ By the Cauchy-Schwarz inequality and the fact that the norms of Markov operators are no greater than one,}
	\begin{equation} \nonumber
		\begin{aligned}
			\|\Mtk \mtkfont{S}^*f\|_{\omega}^2 =& c^2 \|\hat{\Mtk} \mtkfont{S}^* f\|_{\omega}^2 + (1-c)^2 \|\mtkfont{R} \mtkfont{S}^* f\|_{\omega}^2 + 2 c(1-c)  \langle \hat{\Mtk} \mtkfont{S}^* f, \mtkfont{R} \mtkfont{S}^* f \rangle_{\omega}  \\
			\leq & c^2 \|\hat{\Mtk} \mtkfont{S}^* f\|_{\omega}^2 + (1-c^2) \|\mtkfont{S}^* f\|_{\omega}^2.
		\end{aligned}
	\end{equation}
	It follows that
	\begin{equation} \label{ine:fnormdecomp-2}
		\|\mtkfont{S}^*f\|_{\omega}^2 - \|\Mtk \mtkfont{S}^* f\|_{\omega}^2 \geq c^2 \left( \|\mtkfont{S}^*f\|_{\omega}^2 - \|\hat{\Mtk } \mtkfont{S}^* f\|_{\omega}^2 \right).
	\end{equation}
	
	Define an operator $E: L_0^2(\omega) \to L_0^2(\omega)$ as
	\begin{equation} \nonumber
		Ef(y) = \sum_{k \in \mathsf{K}} 1_{\Y_k}(y) \frac{1}{\omega(\Y_k)} \int_{\Y_k} \omega(\df y') f(y') = \sum_{k \in \mathsf{K}} 1_{\Y_k}(y) \int_{\Y_k} \omega_k(\df y') f(y').
	\end{equation}
	Then~$E$ is an orthogonal projection: $E^2 = E^* = E$.
	Its range is the set of $L^2_0(\omega)$ functions that are constant on any given $\Y_k$.
	It is easy to see that $\hat{\Mtk }E = E$.
	In fact, $E\hat{\Mtk } = E$ as well.
	To see this, note that, for $g \in L_0^2(\omega)$,
	\[
	\begin{aligned}
		E \hat{\Mtk } g(y) &= \sum_{k \in \mathsf{K}} 1_{\Y_k}(y) \int_{\Y_k} \omega_k(\df y') \int_{\Y_k} \Mtk _k(y', \df y'') g(y'') \\
		&= \sum_{k \in \mathsf{K}} 1_{\Y_k}(y) \int_{\Y_k} \omega_k(\df y'') g(y'').
	\end{aligned}
	\]
	It then follows that 
	\begin{equation} \label{eq:ET}
		\begin{aligned}
			\|\mtkfont{S}^*f\|_{\omega}^2 - \|\hat{\Mtk } \mtkfont{S}^* f\|_{\omega}^2 =& \|[E + (I-E)] \mtkfont{S}^*f\|_{\omega}^2 - \|\hat{\Mtk } [E + (I-E)] \mtkfont{S}^* f\|_{\omega}^2 \\
			=& \|E\mtkfont{S}^*f \|_{\omega}^2 + \|(I-E) \mtkfont{S}^* f\|_{\omega}^2 - \|\hat{\Mtk } E \mtkfont{S}^* f \|_{\omega}^2 - \|\hat{\Mtk } (I-E) \mtkfont{S}^* f \|_{\omega}^2 - \\
			& 2 \langle \hat{\Mtk } E \mtkfont{S}^*f, \hat{\Mtk } (I - E)  \mtkfont{S}^* f \rangle_{\omega} \\
			=& \|(I-E) \mtkfont{S}^* f\|_{\omega}^2 - \|\hat{\Mtk } (I-E) \mtkfont{S}^* f \|_{\omega}^2.
		\end{aligned}
	\end{equation}
	Here, $I$ is the identity operator.
	{ We have used the following properties of an orthogonal projection: $E = E^*$ and $(I-E)E = E(I-E) = 0$.}
	
	
	For $k \in \mathsf{K}$, let $M_k \mtkfont{S}^* f: \Y_k \to \mathbb{R}$ be such that
	\[
	M_k \mtkfont{S}^* f (y) = (I-E) \mtkfont{S}^* f(y) = \mtkfont{S}^* f(y) - \int_{\Y_k} \omega_k(\df y') \mtkfont{S}^* f(y')
	\]
	for $y \in \Y_k$.
	Then $M_k\mtkfont{S}^* f$ is in $L_0^2(\omega_k)$, and
	\[
	\|(I - E) \mtkfont{S}^* f \|_{\omega}^2 = \sum_{k \in \mathsf{K}} \int_{\Y_k} \omega(\df y) \, [M_k \mtkfont{S}^* f(y)]^2 = \sum_{k \in \mathsf{K}} \omega(\Y_k) \|M_k \mtkfont{S}^* f \|_{\omega_k}^2.
	\]
	Moreover,
	\begin{equation} \label{ine:fnormdecomp-3}
		\begin{aligned}
			\|\hat{\Mtk } (I-E) \mtkfont{S}^* f \|_{\omega}^2 &= \sum_{k \in \mathsf{K}} \int_{\Y_k} \omega(\df y) \, [\Mtk _k M_k \mtkfont{S}^* f(y)]^2 \\
			&= \sum_{k \in \mathsf{K}} \omega(\Y_k) \|\Mtk _k M_k \mtkfont{S}^* f \|_{\omega_k}^2 \\
			&\leq \left( \sup_{k \in \mathsf{K}} \|\Mtk _k\|_{\omega_k}^2 \right) \|(I - E) \mtkfont{S}^* f \|_{\omega}^2 .
		\end{aligned}
	\end{equation}
	
	Note that $\|\mtkfont{S}^*\|_{\omega} = \|\mtkfont{S} \|_{\omega} \leq 1$, so $\|f\|_{\omega}^2 - \|\mtkfont{S}^*f\|_{\omega}^2 \geq 0$.
	Combining this fact with~\eqref{eq:fnormdecomp-1} to~\eqref{ine:fnormdecomp-3} yields
	\[
	\begin{aligned}
		\|f\|_{\omega}^2 - \|\Mtk \mtkfont{S}^* f\|_{\omega}^2 & \geq \|f\|_{\omega}^2 - \|\mtkfont{S}^*f\|_{\omega}^2 + c^2 \left( 1 - \sup_{k \in \mathsf{K}} \|\Mtk _k\|_{\omega_k}^2 \right) \|(I - E) \mtkfont{S}^* f \|_{\omega}^2 \\
		& \geq c^2 \left( 1 - \sup_{k \in \mathsf{K}} \|\Mtk _k\|_{\omega_k}^2 \right) \left[ \|f\|_{\omega}^2 - \|\mtkfont{S}^*f\|_{\omega}^2 + \|(I - E) \mtkfont{S}^* f \|_{\omega}^2  \right] \\
		&= c^2 \left( 1 - \sup_{k \in \mathsf{K}} \|\Mtk _k\|_{\omega_k}^2 \right) \left[ \|f\|_{\omega}^2 - \|E \mtkfont{S}^*f\|_{\omega}^2  \right] \\
		&\geq c^2 \left( 1 - \sup_{k \in \mathsf{K}} \|\Mtk _k\|_{\omega_k}^2 \right) (1 - \|E\mtkfont{S}^*\|_{\omega}^2) \|f\|_{\omega}^2.
	\end{aligned}
	\]
	Since~$f$ is arbitrary,
	\[
	1 - \|\Mtk \mtkfont{S}^*\|_{\omega}^2 \geq c^2 \left( 1 - \sup_{k \in \mathsf{K}} \|\Mtk _k\|_{\omega_k}^2 \right) (1 - \|E\mtkfont{S}^*\|_{\omega}^2).
	\]
	
	To complete the proof, it suffices to show that $\|E\mtkfont{S}^*\|_{\omega}^2 = \|\overline{\mtkfont{S}^* \mtkfont{S}}\|_{\bar{\omega}}$.
	Note that $\|ES^*\|_{\omega} = \|(E\mtkfont{S}^*)^*\|_{\omega} = \|\mtkfont{S}E\|_{\omega}$.
	Denote the range of~$E$ by~$H$.
	Then
	\begin{equation} \label{eq:SEnorm-1}
		\|E\mtkfont{S}^*\|_{\omega}^2 = \|\mtkfont{S}E\|_{\omega}^2 = \sup_{f \in L_0^2(\omega) \setminus \{0\}} \frac{\|\mtkfont{S}Ef\|_{\omega}^2}{\|Ef\|_{\omega}^2 + \|(I-E)f\|_{\omega}^2} =  \sup_{f \in H \setminus \{0\}} \frac{\|\mtkfont{S} f\|_{\omega}^2}{\|f\|_{\omega}^2}.
	\end{equation}
	The transformation that maps a function $g: \mathsf{K} \to \mathbb{R}$ to $\sum_{k \in \mathsf{K}} g(k) 1_{\Y_k}$
	defines an isomorphism from $L_0^2(\bar{\omega})$ to~$H$.
	Call this transformation~$U$.
	Because~$U$ is unitary,
	\begin{equation} \label{eq:SEnorm-2}
		\sup_{f \in H \setminus \{0\}} \frac{\|\mtkfont{S}f\|_{\omega}^2}{\|f\|_{\omega}^2} =  \sup_{g \in L_0^2(\bar{\omega})  \setminus \{0\}} \frac{\|\mtkfont{S} Ug\|_{\omega}^2}{\|g\|_{\bar{\omega}}^2}.
	\end{equation} 
	For $g \in L_0^2(\bar{\omega})$,
	\begin{equation} \label{eq:SEnorm-3}
		\|\mtkfont{S} Ug\|_{\omega}^2 = \int_{\Y} \omega(\df y) \left[ \sum_{k \in \mathsf{K}} \mtkfont{S}(y, \Y_k) g(k) \right]^2 = \langle g, \overline{\mtkfont{S}^*\mtkfont{S}} g \rangle_{\bar{\omega}}.
	\end{equation}
	Note that $\overline{\mtkfont{S}^* \mtkfont{S}}$ defines a positive semi-definite operator on $L_0^2(\bar{\omega})$.
	Then, by~\eqref{eq:SEnorm-1},~\eqref{eq:SEnorm-2}, and~\eqref{eq:SEnorm-3}, $\|E\mtkfont{S}^*\|_{\omega}^2 = \|\overline{\mtkfont{S}^* \mtkfont{S}}\|_{\bar{\omega}}$ as desired.
\end{proof}

%{\color{red}
%The proof above takes inspiration from the proof of the original Markov chain decomposition result for reversible chains, given in \cite{madras2002markov}.
%We follow the earlier proof's idea of decomposing functions in $L_0^2(\omega)$ into components in~$H$ and components orthogonal to~$H$.
%The key difference that allows for the extension to the non-reversible case is that while \cite{madras2002markov} worked with Dirichlet forms, we work with squared norms.
%For instance, instead of \eqref{eq:fnormdecomp-1}, \cite{madras2002markov} considered the formula
%\[
%\langle f ,  (I - \mtkfont{S}^{1/2} \Mtk \mtkfont{S}^{1/2} ) f\rangle_{\omega} = \langle f, (I-\mtkfont{S}) f\rangle_{\omega} + \langle f ,  (\mtkfont{S}-\mtkfont{S}^{1/2} \Mtk \mtkfont{S}^{1/2}) f\rangle_{\omega} ;
%\]
%instead of \eqref{ine:fnormdecomp-2}, \cite{madras2002markov} has (up to minor modifications)
%\[
%\langle \mtkfont{S}^{1/2} f, (I - \Mtk) \mtkfont{S}^{1/2} f \rangle_{\omega} \geq c \, \langle \mtkfont{S}^{1/2} f, (I - \hat{\Mtk}) \mtkfont{S}^{1/2} f \rangle_{\omega}.
%\]
%}

{
Finally, we derive the assertion in Remark \ref{rem:lower}:
$
1 - \|\Mtk\|_{\omega}^2 \leq \mbox{Gap}_{\bar{\omega}}(\overline{\Mtk^* \Mtk}) \leq 1 - \|\bar{\mtkfont{T}}\|_{\bar{\omega}}^2.
$
The result trivially holds if $|\mathsf{K}| = 1$ since, in this case, $ \mbox{Gap}_{\bar{\omega}}(\overline{\Mtk^* \Mtk}) =  1$ and $\|\bar{\Mtk}\|_{\bar{\omega}} = 0$.
Assume that $|\mathsf{K}| > 1$.
Then the range of the projection $E$ is not just $\{0\}$.
By \eqref{eq:SEnorm-2} and \eqref{eq:SEnorm-3},
\[
\|\mtkfont{S}\|_{\omega}^2 \geq \sup_{f \in H \setminus \{0\}} \frac{\|\mtkfont{S} f\|_{\omega}^2}{\|f\|_{\omega}^2} = \|\overline{\mtkfont{S}^* \mtkfont{S}}\|_{\bar{\omega}} = 1 - \mbox{Gap}_{\bar{\omega}} ( \overline{\mtkfont{S}^* \mtkfont{S}} ).
\]
Moreover, by the Cauchy-Schwarz inequality,
\[
\begin{aligned}
	\|\bar{\mtkfont{S}}\|_{\bar{\omega}}^2 &= \sup_{g \in L_0^2(\bar{\omega})  \setminus \{0\}} \|g\|_{\bar{\omega}}^{-2} \sum_{k \in \mathsf{K}} \bar{\omega}(\{k\}) \left[  \int_{\Y_k} \omega_k(\df y) \sum_{k' \in \mathsf{K}} S(y, \Y_{k'}) g(k') \right]^2 \\
	&\leq \sup_{g \in L_0^2(\bar{\omega}) \setminus \{0\}} \|g\|_{\bar{\omega}}^{-2} \sum_{k \in \mathsf{K}} \bar{\omega}(\{k\})   \int_{\Y_k} \omega_k(\df y) \left[ \sum_{k' \in \mathsf{K}} S(y, \Y_{k'}) g(k') \right]^2 \\
	&= \sup_{g \in L_0^2(\bar{\omega}) \setminus \{0\}} \frac{ \langle g, \overline{\mtkfont{S}^*\mtkfont{S}} g \rangle_{\bar{\omega}} }{\|g\|_{\bar{\omega}}^2} \\
	&= \|\overline{S^*S}\|_{\bar{\omega}}.
\end{aligned}
\]
Letting $\mtkfont{S} = \Mtk$ yields the desired inequalities.
}







{
\section{Proof of Lemma~\ref{lem:reordering}} \label{app:reordering-invariant}
}

Let $k \in \mathsf{K}$ be fixed.
First we note that the un-normalized posterior density associated with $\Phi_k$ can be written into the form
\[
\phi_k(\alpha, w, \tau, u) = C_k \tilde{\psi}_k(\alpha, w, \tau, u) \, k! \, \ind_{\mathsf{G}_k}(u),
\]
where $C_k$ is a normalizing constant, and 
\[
\begin{aligned}
	\tilde{\psi}_k(\alpha, w, \tau, u) =& \left\{ \prod_{i=1}^n w_{\alpha_i} \frac{1}{\sqrt{2\bm{\pi} \tau_{\alpha_i} } } \exp\left[ - \frac{1}{2 \tau_{\alpha_i}} (y_i - u_{\alpha_i})^2 \right] \right\} \\
	& \left\{ \prod_{j=1}^k w_j^{\gamma - 1} \frac{b^c}{\Gamma(c)} \tau_j^{-c - 1} e^{-b/\tau_j} \frac{1}{\sqrt{2 \bm{\pi} \tau_0 \tau_j}} \exp\left[ - \frac{(u_j-u_0)^2}{2\tau_0 \tau_j}  \right] \ind_{(0,\infty)}(\tau_j) \right\} \\
	&  \frac{\Gamma(k\gamma)}{\Gamma(\gamma)^k} \ind_{\mathsf{S}_k}(w)
\end{aligned}
\]
It can be checked that $C_k \tilde{\psi}_k(\cdot)$ is a probability density function that is invariant under label switching.
A label switch associated with a permutation $\sigma: \{1,\dots,k\} \to \{1,\dots,k\}$ is a one-to-one function on $\tilde{\Z}_k := \{1,\dots,k\}^n \times \mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k$ that results from re-labeling the $j$th Gaussian component in the mixture model as the $\sigma(j)$'th component for $j = 1,\dots,k$.
To be precise, when applied to $(\alpha, w, \tau, u) \in \tilde{\Z}_k$, it maps $\alpha = (\alpha_1, \dots, \alpha_n)$ to $L_{\sigma,1} (\alpha) := (\sigma(\alpha_1), \dots, \sigma(\alpha_n))$, $w = (w_1, \dots, w_k)$ to $L_{\sigma,2}(w) := (w_{\sigma^{-1}(1)}, \dots, w_{\sigma^{-1}(k)})$, $\tau$ to $L_{\sigma,2}(\tau)$ and $u$ to $L_{\sigma,2}(u)$.
We shall use $\tilde{\Phi}_k$ to denote the distribution associated with $C_k \tilde{\psi}_k(\cdot)$.

In the Metropolis re-ordering algorithm, with some abuse of notation, denote the current state by $(A,W,T,U)$, and denote the random elements generated by steps 1 to 6 by $A^{(0)}$, $A''$, $W''$, $T''$, $U''$, $(A',W',T',U')$, respectively.
Assume that $(A,W,T,U) \sim \Phi_k$.
The goal is to show that $(A',W',T',U') \sim \Phi_k$.

Denote the collection of permutations on $\{1,\dots,k\}$ by $\Sigma_k$.
By the invariance of $\tilde{\psi}_k$ under label switching, for $\mathsf{B} \subset \{1,\dots,k\}^n$, 
\[
\begin{aligned}
	\mbox{Pr}(A^{(0)} \in \mathsf{B}) 
	=& \frac{1}{k!} \sum_{\sigma \in \Sigma_k} \mbox{Pr}(L_{\sigma,1}(A) \in \mathsf{B}) \\
	=&  C_k \sum_{\sigma \in \Sigma_k} \sum_{\alpha \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k} \int_{(0,\infty)^k} \int_{\mathbb{R}^k} \ind_{\mathsf{B}}(L_{\sigma,1}(\alpha)) \, \ind_{\mathsf{G}_k}(u) \, \tilde{\psi}_k(\alpha, w, \tau, u) \, \df u \, \df \tau \, \df w \\
	=&  C_k \sum_{\sigma \in \Sigma_k} \sum_{\alpha \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k} \int_{(0,\infty)^k} \int_{\mathbb{R}^k} \ind_{\mathsf{B}}(L_{\sigma,1}(\alpha)) \, \ind_{L_{\sigma,2}(\mathsf{G}_k)}(L_{\sigma,2}(u)) \\
	& \tilde{\psi}_k(L_{\sigma,1}(\alpha), L_{\sigma,2}(w), L_{\sigma,2}(\tau), L_{\sigma,2}(u)) \, \df u \, \df \tau \, \df w \\
	=&  C_k \sum_{\sigma \in \Sigma_k} \sum_{\tilde{\alpha} \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k} \int_{(0,\infty)^k} \int_{\mathbb{R}^k} \ind_{\mathsf{B}}(\tilde{\alpha}) \, \ind_{L_{\sigma,2} (\mathsf{G}_k)}(\tilde{u}) \, \tilde{\psi}_k(\tilde{\alpha}, \tilde{w}, \tilde{\tau}, \tilde{u}) \, \df \tilde{u} \, \df \tilde{\tau} \, \df \tilde{w} \\
	=& C_k \sum_{\tilde{\alpha} \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k} \int_{(0,\infty)^k} \int_{\mathbb{R}^k} \ind_{\mathsf{B}}(\tilde{\alpha}) \, \tilde{\psi}_k(\tilde{\alpha}, \tilde{w}, \tilde{\tau}, \tilde{u}) \, \df \tilde{u} \, \df \tilde{\tau} \, \df \tilde{w}.
\end{aligned}
\]
Note that we have utilized the fact that the determinant of $L_{\sigma,2}$ is~1, and that $\sum_{\sigma} \ind_{L_{\sigma,2}(\mathsf{G}_k)}(\tilde{u}) = 1$ for $\tilde{u} \in \mathbb{R}^k$ outside a Lebesgue measure zero set.
The above display shows that $A^{(0)}$ has the same distribution as $\tilde{A}$ if $(\tilde{A}, \tilde{W}, \tilde{T}, \tilde{U}) \sim \tilde{\Phi}_k$.
In fact, it is easy to see that the probability mass function of this distribution is precisely $\tilde{\pi}_A(\cdot \mid k, y)$.
Step 2 in the Metropolis re-ordering algorithm is but a sequence of Metropolis Hastings steps targeting this distribution.
Thus, the distribution of $A''$ remains the same.
It can be checked that, given $A''$,  $(W'', T'', U'')$ follows precisely the conditional distribution of $(\tilde{W}, \tilde{T}, \tilde{U})$ given $\tilde{A}$ if $(\tilde{A}, \tilde{W}, \tilde{T}, \tilde{U}) \sim \tilde{\Phi}_k$.
Therefore, $(A'', W'', T'', U'') \sim \tilde{\Phi}_k$.
Again by the label-switching invariance, for $\mathsf{B} \in \Z_k$,
\[
\begin{aligned}
	& \mbox{Pr}((A',W',T',U') \in \mathsf{B}) \\
	=& \sum_{\sigma \in \Sigma_k} \mbox{Pr} ( (L_{\sigma,1}(A''), L_{\sigma,2}(W''), L_{\sigma,2}(T''), L_{\sigma,2}(U'')) \in B, \; L_{\sigma,2}(U'') \in \mathsf{G}_k ) \\
	=& C_k \sum_{\sigma \in \Sigma_k} \sum_{\alpha'' \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k} \int_{(0,\infty)^k} \int_{\mathbb{R}^k} \ind_{\mathsf{B}}(L_{\sigma,1}(\alpha''), L_{\sigma,2}(w''), L_{\sigma,2}(\tau''), L_{\sigma,2}(u'') ) \, \ind_{\mathsf{G}_k}(L_{\sigma,2}(u'')) \\
	& \tilde{\psi}_k(\alpha'', w'', \tau'', u'') \, \df u'' \, \df \tau'' \, \df w'' \\
	=& C_k \sum_{\sigma \in \Sigma_k} \sum_{\alpha' \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k} \int_{(0,\infty)^k} \int_{\mathbb{R}^k} \ind_{\mathsf{B}} (\alpha', w', \tau', u') \, \ind_{\mathsf{G}_k}(u') \, \tilde{\psi}_k(\alpha', w', \tau', u') \, \df u' \, \df \tau' \, \df w' \\
	=& \Phi_k(\mathsf{B}).
\end{aligned}
\]
This completes the proof.


%		
%		Next, recall that one iteration of the algorithm consists of 5 steps.
%		Conditioning on the current state $(A,W,T,U)$, steps 1 to 4 generate the random elements $A'', W'', T'', U''$, respectively, and step 5 yields the next state $(A', W', T', U')$.
%		For a bounded function $f: \Z_k \to \mathbb{R}$, let $Q_5 f: \Z_k \to \mathbb{R}$ be such that $Q_5 f (A'',W'',T'',U'') = E[f(A',W',T',U') \mid A'',W'',T'',U'']$;
%		let $Q_{2,3,4} P_5 f: \{1,\dots,k\}^n \to \mathbb{R}$ be such that $Q_{2,3,4} P_5 f(A'') = E[f(A',W',T',U') \mid A'']$;
%		let $Q_1 Q_{2,3,4} Q_5 f: \{1,\dots,k\}^n \to \mathbb{R}$ be such that $Q_1 Q_{2,3,4} Q_5 f(A) = E[f(A',W',T',U') \mid A]$.
%		We will show that if $\Phi_k f = 0$ and $(A, W, T, U) \sim \Phi_k$, then $E [Q_1 Q_{2,3,4} Q_5 f(A)] = 0$.
%		Note that this is enough to guarantee that the algorithm leaves $\Phi_k$ invariant.
%		
%		Let $f: \Z_k \to \mathbb{R}$ be bounded and assume that $\Phi_k f = 0$.
%		For $\alpha'' \in \{1,\dots,k\}^n$ and $(w'',\tau'',u'') \in \mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k$ outside a Lebesgue measure zero set,
%		\[
%		Q_5 f (\alpha'', w'', \tau'', u'') = \sum_{\sigma \in \Sigma_k} f \circ L_{\sigma}(\alpha'', w'', \tau'', u'') \, \ind_{\mathsf{G}_k}(L_{\sigma,2}(u'')),
%		\]
%		where $\Sigma_k$ is the collection of permutations on $\{1,\dots,k\}$.
%		Note that $Q_5 f(\cdot)$ is invariant under label switching, and, since $\tilde{\psi}_k$ is also invariant under label switching,
%		\begin{equation} \label{eq:Q5zero}
	%		\begin{aligned}
		%			& \sum_{\alpha'' \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k } Q_5 f (\alpha'', w'', \tau'', u'') \, \tilde{\psi}_k(\alpha'', w'', \tau'', u'') \, \df (w'', \tau'', u'') \\
		%			=& \sum_{\alpha'' \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k } \sum_{\sigma \in \Sigma_k} f \circ L_{\sigma}(\alpha'', w'', \tau'', u'') \, \ind_{\mathsf{G}_k}(L_{\sigma,2}(u'')) \\
		%			& \tilde{\psi}_k \circ L_{\sigma} (\alpha'', w'', \tau'', u'') \, \df (w'', \tau'', u'') \\
		%			\propto & \sum_{\sigma \in \Sigma_k} \sum_{\bar{\alpha} \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k } f(\bar{\alpha}, \bar{w}, \bar{\tau}, \bar{u}) \, \psi_k(\bar{\alpha}, \bar{w}, \bar{\tau}, \bar{u}) \, \df (\bar{w}, \bar{\tau}, \bar{u}) = 0.
		%		\end{aligned}
	%		\end{equation}
%		where the last line follows from the volume-preserving transformation $(\bar{\alpha}, \bar{w}, \bar{\tau}, \bar{u}) = L_{\sigma}(\alpha'',w'',\tau'',u'')$.
%		
%		For $\alpha'' \in \{1,\dots,k\}^n$, 
%		\[
%		Q_{2,3,4} Q_5 f(\alpha'') = \frac{1}{\tilde{\pi}_A(\alpha'' \mid k,y)} \int_{\mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k } Q_5 f(\alpha'', w'', \tau'', u'') \, \tilde{\psi}_k(\alpha'', w'', \tau'', u'') \, \df (w'',\tau'',u''),
%		\]
%		where $\tilde{\pi}_A(\alpha''\mid k,y)$ is $\tilde{\psi}_k(\alpha'', w'', \tau'', u'')$ with $(w'',\tau'',u'')$ integrated out.
%		Then, by \eqref{eq:Q5zero},
%		\begin{equation} \label{eq:Q234zero}
	%		\sum_{\alpha'' \in \{1,\dots,k\}^n } \tilde{\pi}_A(\alpha'') \, Q_{2,3,4} Q_5 f(\alpha'') = 0.
	%		\end{equation}
%		Since $Q_5 f(\cdot)$, $\tilde{\psi}_k(\cdot)$ and, by extension, $\tilde{\pi}_A(\cdot \mid k,y)$ are all invariant under variable switching, so is $Q_{2,3,4} Q_5 f(\cdot)$.
%		
%		Finally, for $\alpha \in \{1,\dots,k\}^n$, 
%		\[
%		\begin{aligned}
	%			Q_1 Q_{2,3,4} Q_5 f(\alpha) =& \frac{1}{nk} \sum_{\bar{i}=1}^n \sum_{\bar{j}=1}^k \bigg[ \left( 1 - \min \left\{ 1, \frac{\tilde{\pi}_A(\alpha_{\bar{i} \leftarrow \bar{j}} \mid k, y)}{\tilde{\pi}_A(\alpha \mid k, y)} \right\} \right) Q_{2,3,4} Q_5 f(\alpha) + \\
	%			& \min \left\{ 1, \frac{\tilde{\pi}_A(\alpha_{\bar{i} \leftarrow \bar{j}} \mid k, y)}{\tilde{\pi}_A(\alpha \mid k, y)} \right\} Q_{2,3,4} Q_5 f (\alpha_{\bar{i} \leftarrow \bar{j}}) \bigg].
	%		\end{aligned}
%		\]
%		Note that for any permutation $\sigma: \{1,\dots,k\}^n \to \{1,\dots,k\}^n$, $L_{\sigma,1}(\alpha_{\bar{i} \leftarrow \bar{j}}) = [L_{\sigma,1}(\alpha)]_{\bar{i} \leftarrow \sigma(\bar{j})}$.
%		Since $\tilde{\pi}_A(\cdot \mid k, y)$ and $Q_{2,3,4} Q_5 f(\cdot)$ are invariant under label switching, for any permutation~$\sigma$,
%		\[
%		\begin{aligned}
	%			Q_1 Q_{2,3,4} Q_5 f \circ L_{\sigma,1} (\alpha) =& \frac{1}{nk} \sum_{\bar{i}=1}^n \sum_{\bar{j}=1}^k \bigg[ \left( 1 - \min \left\{ 1, \frac{\tilde{\pi}_A(\alpha_{\bar{i} \leftarrow \sigma (\bar{j})} \mid k, y)}{\tilde{\pi}_A(\alpha \mid k, y)} \right\} \right) Q_{2,3,4} Q_5 f(\alpha) + \\
	%			& \min \left\{ 1, \frac{\tilde{\pi}_A(\alpha_{\bar{i} \leftarrow \sigma (\bar{j})} \mid k, y)}{\tilde{\pi}_A(\alpha \mid k, y)} \right\} Q_{2,3,4} Q_5 f (\alpha_{\bar{i} \leftarrow \sigma (\bar{j})}) \bigg] \\
	%			=& Q_1 Q_{2,3,4} Q_5 f(\alpha),
	%		\end{aligned}
%		\]
%		where the last line holds because summing out $\bar{j}$ is equivalent to summing out $\sigma(\bar{j})$.
%		That is, $Q_1 Q_{2,3,4} Q_5 f(\cdot)$ is invariant under label switching.
%		Moreover, step 1 corresponds to a Metropolis Hastings algorithm targeting the measure associated with $\tilde{\pi}_A(\cdot)$, so by \eqref{eq:Q234zero},
%		\[
%		\sum_{\alpha \in \{1,\dots,k\}^n} \tilde{\pi}_A(\alpha) \,  Q_1 Q_{2,3,4} Q_5 f(\alpha) = \sum_{\alpha \in \{1,\dots,k\}^n} \tilde{\pi}_A(\alpha) \,  Q_{2,3,4} Q_5 f(\alpha) = 0.
%		\]
%		On the other hand, by the invariance of $\tilde{\psi}_k(\cdot)$ and $Q_1 Q_{2,3,4} Q_5 f(\cdot)$ under label switching,
%		\[
%		\begin{aligned}
	%			& \sum_{\alpha \in \{1,\dots,k\}^n} \tilde{\pi}_A(\alpha) \,  Q_1 Q_{2,3,4} Q_5 f(\alpha) \\
	%			=& \sum_{\sigma \in \Sigma_k} \sum_{\alpha \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k} \tilde{\psi}_k(\alpha, w, \tau, u) \, \ind_{\mathsf{G}_k} \circ L_{\sigma,2} (u) \, \df (w, \tau, u) \, Q_1 Q_{2,3,4} Q_5 f(\alpha) \\
	%			= & \sum_{\sigma \in \Sigma_k} \sum_{\alpha \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k} \tilde{\psi}_k \circ L_{\sigma} (\alpha, w, \tau, u) \, \ind_{\mathsf{G}_k} \circ L_{\sigma,2} (u) \, \df (w, \tau, u) \, Q_1 Q_{2,3,4} Q_5 f \circ L_{\sigma, 1}(\alpha) \\
	%			= &  \sum_{\alpha \in \{1,\dots,k\}^n} \int_{\mathsf{S}_k \times (0,\infty)^k \times \mathbb{R}^k} \psi_k(\alpha, w, \tau, u) \, \df (w,\tau,u) \, Q_1 Q_{2,3,4} f(\alpha) \\
	%			\propto & \, E[Q_1 Q_{2,3,4} Q_5 f(A)],
	%		\end{aligned}
%		\]
%		where $(A,W,T,U) \sim \Phi_k$.
%		Combining the two most recent displays then shows that $E[Q_1 Q_{2,3,4} Q_5 f(A)] = 0$.

{
\section{Proof of Lemma~\ref{lem:reordering-geo}} \label{app:reordering-geo}
}

Let $f \in L_0^2(\Phi_k)$ be arbitrary.
Then by Lemma \ref{lem:reordering}, $P_k f \in L_0^2(\Phi_k)$, and $\|P_k f\|_{\Phi_k} \leq \|f\|_{\Phi_k}$.
Denote by $\mathcal{H}_A$ the subspace of functions in $L_0^2(\Phi_k)$ that depend on $(\alpha, w, \tau, u)$ only through~$\alpha$.
Then $P_k f \in \mathcal{H}_A$ since the measure $P_k((\alpha,w,\tau,u), \cdot)$ depends on $(\alpha,w,\tau,u)$ only through~$\alpha$.
It follows that $\|P_k^2 f\|_{\Phi_k} \leq \|P_k|_{\mathcal{H}_A}\|_{\Phi_k} \|P_k f\|_{\Phi_k} \leq \|P_k|_{\mathcal{H}_A}\|_{\Phi_k} \|f\|_{\Phi_k}$, where $P_k |_{\mathcal{H}_A}: \mathcal{H}_A \to \mathcal{H}_A$ is $P_k$ restricted to $\mathcal{H}_A$, and $\|P_k|_{\mathcal{H}_A}\|_{\Phi_k} = \sup_g \|P_k g\|_{\Phi_k}/\|g\|_{\Phi_k}$ where the supremum is taken over $\mathcal{H}_A \setminus \{0\}$.
To prove $\|P_k^2\|_{\Phi_k} < 1$, it suffice to show that $\|P_k |_{\mathcal{H}_A} \|_{\Phi_k} < 1$.
Since $\mathcal{H}_A$ is finite dimensional, bounded linear operators on $\mathcal{H}_A$ must be norm-attaining.
Thus, to show that  $\|P_k |_{\mathcal{H}_A} \|_{\Phi_k} < 1$, it suffices to show that $\|P_k g \|_{\Phi_k} < \|g\|_{\Phi_k}$ for any $g \in \mathcal{H}_A \setminus \{0\}$.

Let $g \in \mathcal{H}_A \setminus \{0\}$ be arbitrary.
Contrary to what we wish to prove, assume that $\|P_k g \|_{\Phi_k} = \|g\|_{\Phi_k}$.
Abusing notations we write $g(\alpha,w,\tau,u)$ as $g(\alpha)$.
Then $P_k g(\alpha) := P_k g(\alpha, w, \tau, u)$ can be written as $\sum_{\alpha' \in \{1,\dots,k\}^n} p(\alpha,\alpha') g(\alpha')$, where $p(\alpha,\alpha') = P_k((\alpha,w,\tau,u), \{\alpha'\} \times \mathsf{S}_k \times (0,\infty)^n \times \mathsf{G}_k)$ for $(w,\tau,u) \in  \mathsf{S}_k \times (0,\infty)^n \times \mathsf{G}_k$.
Let $\pi_A(\alpha) = \int_{\mathsf{S}_k \times (0,\infty)^n \times \mathsf{G}_k} \phi_k(\alpha,w,\tau,u) \, \df(w,\tau,u)$ for $\alpha \in \{1,\dots,k\}^n$, where $\phi_k$ is the density of $\Phi_k$.
Then $\pi_A(\alpha) > 0$ for $\alpha \in \{1,\dots,k\}^n$, and by Lemma \ref{lem:reordering}, $\sum_{\alpha \in \{1,\dots,k\}^n} \pi_A(\alpha) \, p(\alpha, \alpha') = \pi_A(\alpha')$ for $\alpha' \in \{1,\dots,k\}^n$.
One can derive derive
\[
\begin{aligned}
	& \|g\|_{\Phi_k}^2 = \sum_{\alpha \in \{1,\dots,k\}^n} \pi_A(\alpha) \sum_{\alpha' \in \{1,\dots,k\}^n} p(\alpha,\alpha') \, g(\alpha')^2, \\
	& \|P_k g\|_{\Phi_k}^2 = \sum_{\alpha \in \{1,\dots,k\}^n} \pi_A(\alpha) \left[ \sum_{\alpha' \in \{1,\dots,k\}^n} p(\alpha,\alpha') \, g(\alpha') \right]^2.
\end{aligned}
\]
Then, by the Cauchy-Schwarz inequality, $\|P_k g\|_{\Phi_k}^2 < \|g\|_{\Phi_k}^2$ unless, for $\alpha \in \{1,\dots,k\}^n$, the function $\alpha' \mapsto \sqrt{p(\alpha, \alpha') } \, g(\alpha')$ is proportional to $\sqrt{p(\alpha,\alpha')}$, i.e., $g(\alpha')$ is constant on the set $\{\alpha': \, p(\alpha,\alpha') > 0\}$.
By the construction of the Metropolis re-ordering algorithm, $p(\alpha,\alpha') > 0$ for $\alpha, \alpha' \in \mathsf{K}$.
Thus, $g$ must be a constant function on $\mathsf{K}$.
But this cannot happen, as $g$ is nonzero, and $\mathcal{H}_A$, as a subset of $L_0^2(\Phi_k)$, does not contain nonzero constant functions.
Therefore, $\|P_k g \|_{\Phi_k} < \|g\|_{\Phi_k}$, which leads to the desired result.

{
\section{Wald confidence intervals} \label{app:wald}
}

As mentioned in Section \ref{sssec:binary-example}, batch means estimators may underestimate the asymptotic variance when the sample size is not sufficiently large.
Closely related to this, Wald confidence intervals may suffer from low coverage if the Monte Carlo sample size is not sufficiently large \citep{flegal2010batch,vollset1993confidence}.
%Various remedies have been proposed when the sample consists of iid observations \citep{vollset1993confidence,newcombe1998two,tobi2005small}.
%But these methods do not extend to dependent samples.
In Section \ref{sssec:binary-example}, we proposed adding $(\log m) \sqrt{1/b_m^2 + b_m/m}$ to the batch means estimates of asymptotic variances, where $m$ is the Monte Carlo sample size, and $b_m$ is the batch size.
We shall apply this technique to the toy example in Section \ref{ssec:toy}.

In the toy example, for $k \in \mathsf{K}$, $\pi_k := \Pi(\{k\} \times \Z_k) = 1/k_{\scriptsize\mbox{max}}$, and $|\Z_k| = n$.
Take $k_{\scriptsize\mbox{max}} = 10$ and $n = 15$, so that $\pi_k = 0.1$ for each~$k$.
We employ the chain with slow local and global moves, which is simulated for $m=5000$ or $m = 5 \times 10^3$ iterations.
Simultaneous $1-\alpha$ Wald confidence intervals are constructed for $(\pi_1, \dots, \pi_{10})$, where $\alpha$ is 0.1 or 0.05.
Bonferroni correction is used.
The estimated asymptotic variance is either inflated by adding $(\log m) \sqrt{1/b_m^2 + b_m/m}$ or unchanged, where $b_m \approx m^{0.6}$.
The experiment is repeated 1000 times and the empirical coverage rate is compared to the nominal converge rate.
The average half-width of the $10 \times 1000$ intervals is also recorded.

The results are given in Table \ref{tab:wald}.
We see that inflating the asymptotic variance significantly enhances the empirical coverage rate when the sample size is insufficient, although the resultant intervals tend to be conservative.


\begin{table} \caption{Empirical simultaneous coverage rates of confidence intervals}\label{tab:wald}
	\centering
	\begin{tabular}{ccccc}
			\hline
			$m$ & $1-\alpha$ & inflated & empirical coverage & mean half-width \\
			\hline
			$5 \times 10^3$  & 0.90 & No & 0.796 & 0.038 \\
			$5 \times 10^3$  & 0.90 & Yes & 0.997 & 0.059 \\
			$5 \times 10^3$  & 0.95 & No & 0.842 & 0.041 \\
			$5 \times 10^3$  & 0.95 & Yes & 0.999 & 0.065 \\
			$5 \times 10^4$  & 0.90 & No & 0.888 & 0.013 \\
			$5 \times 10^4$  & 0.90 & Yes & 0.996 & 0.018 \\
			$5 \times 10^4$  & 0.95 & No & 0.935 & 0.014 \\
			$5 \times 10^4$  & 0.95 & Yes & 0.999 & 0.020 \\
			\hline
		\end{tabular}
\end{table}

%\begin{table} \caption{Empirical coverage rates of confidence intervals}\label{tab:wald}
%	\centering
%	\begin{tabular}{ccccccc}
%		\hline
%		$m$ & $k$ & $\pi_k$ & nominal coverage & inflated & empirical coverage & average half-width \\
%		\hline
%		$10^4$ & 1 & 0.018 & 0.95 & No & 0.870 & 0.014 \\
%		$10^4$ & 1 & 0.018 & 0.95 & Yes & 1.000 & 0.028 \\
%		$10^4$ & 10 & 0.182 & 0.95 & No & 0.896 & 0.045 \\
%		$10^4$ & 10 & 0.182 & 0.95 & Yes & 0.939 & 0.051 \\
%		$10^4$ & 1 & 0.018 & 0.99 & No & 0.920 & 0.018 \\
%		$10^4$ & 1 & 0.018 & 0.99 & Yes & 1.000 & 0.036 \\
%		$10^4$ & 10 & 0.182 & 0.99 & No & 0.969 & 0.059 \\
%		$10^4$ & 10 & 0.182 & 0.99 & Yes & 0.985 & 0.067 \\
%		$4 \times 10^4$ & 1 & 0.018 & 0.95 & No & 0.921 & 0.0074 \\
%		$4 \times 10^4$ & 1 & 0.018 & 0.95 & Yes & 1.000 & 0.013 \\
%		$4 \times 10^4$ & 10 & 0.182 & 0.95 & No & 0.935 & 0.024 \\
%		$4 \times 10^4$ & 10 & 0.182 & 0.95 & Yes & 0.961 & 0.026 \\
%		$4 \times 10^4$ & 1 & 0.018 & 0.99 & No & 0.978 & 0.0098 \\
%		$4 \times 10^4$ & 1 & 0.018 & 0.99 & Yes & 1.000 & 0.018 \\
%		$4 \times 10^4$ & 10 & 0.182 & 0.99 & No & 0.985 & 0.032 \\
%		$4 \times 10^4$ & 10 & 0.182 & 0.99 & Yes & 0.992 & 0.035 \\
%		\hline
%	\end{tabular}
%\end{table}
%
%In the toy example in Section \ref{ssec:toy}, for $k \in \mathsf{K}$, $\pi_k := \Pi(\{k\} \times \Z_k) = k/[(k_{\scriptsize\mbox{max}}+1) k_{\scriptsize\mbox{max}}/2]$.
%Take $k_{\scriptsize\mbox{max}} = 10$.
%We employ the trans-dimensional chain associated with the Mtk $P_{(1)}$, which is simulated for $m = 10^4$ or $m = 4 \times 10^4$ iterations.
%A Wald confidence interval is then constructed for $\pi_k$, where $k$ is either~1 or~10.
%The estimated asymptotic variance is either inflated by adding $(\log m) \sqrt{1/b_m^2 + b_m/m}$ or unchanged, where $b_m \approx m^{0.6}$.
%The experiment is repeated 1000 times and the empirical coverage rate is compared to the nominal converge rate.
%The average half-width of the intervals are also recorded.
%
%
%
%The results are given in Table \ref{tab:wald}.
%We see that inflating the asymptotic variance significantly enhances the empirical coverage rate when the sample size is insufficient.
%On the other hand, the procedure tends to be overly conservative when $\pi_k$ is very small.


%
%\section{Ergodicity in an almost sure sense} \label{app:ae}
%
%Let $(\Y, \F, \omega)$ be a generic probability space and suppose that there is an Mtk~$\Mtk $ such that $\omega = \omega \Mtk $.
%The following lemma states that if $\Mtk$ is $\omega$-a.e. geometrically ergodic, then a Markov chain associated with $\Mtk$ can be viewed as Harris ergodic chain if it is started from a distribution concentrated outside an $\omega$-zero measure set.
%This result can be viewed as an extension of Theorem 9.0.1 of \cite{meyn2012markov}.
%
%\begin{lemma} \label{lem:ae}
%	Suppose that there exists a function $G: \Y \times \{0,1,\dots\} \to [0,\infty)$ such that for $\omega$-a.e. $y \in \Y$ and $t \geq 0$, 
%	\begin{equation} \label{ine:ae-1}
%		\|\Mtk ^t(y,\cdot) - \omega(\cdot) \|_{\scriptsize\mbox{TV}} \leq G(y,t).
%	\end{equation}
%	
%	Then there exists $\mathsf{M} \in \F$ such that $\omega(\mathsf{M}) = 0$, $T(y, \mathsf{M}) = 0$ for $y \in \mathsf{M}^c$, and that \eqref{ine:ae-1} holds for $y \in \mathsf{M}^c$ and $t \geq 0$.
%	
%	Moreover, there exists an Mtk $\hat{\Mtk }: \Y \times \F \to [0,1]$ such that (i) For $\mathsf{A}, \mathsf{B} \in \F$, $\int_{\mathsf{A}} \omega(\df y) \Mtk (y, \mathsf{B}) = \int_{\mathsf{A}} \omega(\df y) \hat{\Mtk }(y, \mathsf{B})$,  (ii) $\|\hat{\Mtk }^t(y, \cdot) - \omega(\cdot) \|_{\scriptsize\mbox{TV}} \leq G(y,t)$ for every $y \in \Y$ and $t \geq 0$, and (iii) $\Mtk ^t(y, \cdot) = \hat{\Mtk }^t(y, \cdot)$ for $y \in \mathsf{M}^c$ and every $t \geq 0$.
%	
%	Assume further that the diagonal set $\mathsf{D} := \{(y,y): \, y \in \Y\}$ is $\F^2$-measurable.
%	Then,  for any distribution~$\mu$ such that $\mu(\mathsf{M}) = 0$, on a suitably rich probability space one can construct two chains $Y(t)_{t=0}^{\infty}$ and $\hat{Y}(t)_{t=0}^{\infty}$ associated with~$\Mtk $ and~$\hat{\Mtk }$ respectively, such that  $Y(0) \sim \mu$, and  $Y(t) = \hat{Y}(t)$ for $t \geq 0$ with probability~1. 
%\end{lemma}
%
%\begin{proof}
%	By assumption, there exists $\mathsf{N}_0 \in \F$ such that $\omega(\mathsf{N}_0) = 0$ and~\eqref{ine:ae-1} holds for $y \in \mathsf{N}_0^c$.
%	Since~$\omega$ is stationary for~$\Mtk $, it must hold that $\Mtk (y, \mathsf{N}_0) = 0$ for $\omega$-a.e. $y \in \Y$.
%	In particular, there exists $\mathsf{N}_1 \supset \mathsf{N}_0$ such that $\omega(\mathsf{N}_1) = 0$, and $\Mtk (y, \mathsf{N}_0) = 0$ for $y \in \mathsf{N}_1^c$.
%	By induction, there is an increasing sequence of sets $(\mathsf{N}_j)_{j=0}^{\infty}$ such that $\omega(\mathsf{N}_{j+1}) = 0$ and $\Mtk (y, \mathsf{N}_j) = 0$ whenever $y \in \mathsf{N}_{j+1}^c$ for $j \geq 0$.
%	Let $\mathsf{M} = \bigcup_{j=0}^{\infty} \mathsf{N}_j$.
%	Then $\omega(\mathsf{M}) = \lim_{j \to \infty} \omega(\mathsf{N}_j) = 0$.
%	For $y \in \mathsf{M}^c = \bigcap_{j=0}^{\infty} \mathsf{N}_{j+1}^c$, $\Mtk (y, \mathsf{N}_j) = 0$ for each~$j$, so $\Mtk (y, \mathsf{M}) = \lim_{j \to \infty} \Mtk (y, \mathsf{N}_j) = 0$.
%	
%	
%	Construct $\hat{\Mtk }$ as follows.
%	For $y \in \mathsf{M}^c$, let $\hat{\Mtk }(y, \cdot) = \Mtk (y, \cdot)$.
%	For $y \in \mathsf{M}$, let $\hat{\Mtk }(y, \cdot) = \omega(\cdot)$.
%	(i) holds since, for $\mathsf{A}, \mathsf{B} \in \F$,
%	\[
%	\int_{\mathsf{A}} \omega(\df y) \Mtk (y, \mathsf{B}) = \int_{\mathsf{A} \cap \mathsf{M}^c} \omega(\df y) \Mtk (y, \mathsf{B})  = \int_{\mathsf{A} \cap \mathsf{M}^c} \omega( \df y) \hat{\Mtk }(y, \mathsf{B}) = \int_{\mathsf{A}} \omega( \df y) \hat{\Mtk }(y, \mathsf{B}).
%	\]
%	Assertion (iii) states that $\Mtk ^t(y, \cdot) = \hat{\Mtk }^t(y, \cdot)$ for every~$t$ whenever $y \in \mathsf{M}^c$.
%	This obviously holds when $t = 1$.
%	Suppose that this holds for a positive integer~$t$.
%	For $y \in \mathsf{M}^c$, it was shown that $\Mtk (y, \mathsf{M}) = \hat{\Mtk }(y, \mathsf{M}) = 0$.
%	Thus, for $y \in \mathsf{M}^c$ and $\mathsf{B} \in \F$, 
%	\[
%	\Mtk ^{t+1}(y, \mathsf{B}) = \int_{\mathsf{M}^c} \Mtk (y, \df y') \Mtk ^t(y', \mathsf{B}) = \int_{\mathsf{M}^c} \hat{\Mtk }(y, \df y') \hat{\Mtk }^t(y', \mathsf{B}) = \hat{\Mtk }^{t+1}(y, \mathsf{B}).
%	\]
%	By induction, (iii) holds.
%	Note that, by (i), for $t \geq 1$, $\hat{\Mtk }^t (y,\cdot) = \omega(\cdot)$ if $y \in \mathsf{M}$, and by (iii), for $t \geq 0$, $\hat{\Mtk }^t(y,\cdot) = \Mtk ^t(y,\cdot)$ if $y \in \mathsf{M}^c$.
%	Moreover,~\eqref{ine:ae-1} holds for $y \in \mathsf{M}^c$.
%	Thus, (ii) holds.
%	
%	It remains to construct $Y(t)$ and $\hat{Y}(t)$.
%	Suppose that $\mu(\mathsf{M}) = 0$.
%	Let $Y(0) \sim \mu$, and set $\hat{Y}(0) = Y(0)$.
%	For $t \geq 0$, given $Y(t)$ and $\hat{Y}(t)$, draw $Y(t+1)$ and $\hat{Y}(t+1)$ using the following procedure:
%	If $Y(t) \in \mathsf{M}^c$ and $Y(t) = \hat{Y}(t)$, draw $Y(t+1)$ from the distribution $\Mtk (Y(t), \cdot)$, and let $\hat{Y}(t+1) = Y(t+1)$; otherwise, draw $Y(t+1)$ from $\Mtk (Y(t), \cdot)$, and independently draw $\hat{Y}(t+1)$ from $\hat{\Mtk }(\hat{Y}(t), \cdot)$.
%	Evidently, $(Y(t), \hat{Y}(t))_{t=0}^{\infty}$ is a Markov chain whose transition kernel is
%	\[
%	\begin{aligned}
%		\tilde{\Mtk }((y,\hat{y}), (\df y', \df \hat{y}')) &= \ind_{\mathsf{D}}(y,\hat{y}) \ind_{\mathsf{M}^c}(y) \Mtk (y, \df y') \delta_{y'}(\df \hat{y}') + [ 1 - \ind_{\mathsf{D}}(y,\hat{y}) \ind_{\mathsf{M}^c}(y) ] \Mtk (y, \df y') \hat{\Mtk }(\hat{y}, \df \hat{y}') \\
%		&= \ind_{\mathsf{D}}(y,\hat{y}) \ind_{\mathsf{M}^c}(\hat{y}) \hat{\Mtk }(\hat{y}, \df \hat{y}') \delta_{\hat{y}'}(\df y') + [ 1 - \ind_{\mathsf{D}}(y,\hat{y}) \ind_{\mathsf{M}^c}(\hat{y}) ] \Mtk (y, \df y') \hat{\Mtk }(\hat{y}, \df \hat{y}'),
%	\end{aligned}
%	\]
%	where $\delta_{y'}$ denotes the point mass at~$y'$.
%	Integrating out~$\hat{y}'$ shows that $(Y(t))_{t=0}^{\infty}$ is a Markov chain whose transition law is~$\Mtk $, and integrating out~$y'$ shows that $(\hat{Y}(t))_{t=0}^{\infty}$ is a Markov chain whose transition law is~$\hat{\Mtk }$.
%	For $t \geq 0$,
%	\[
%	\begin{aligned}
%		\mbox{Pr}(Y(t'+1) = \hat{Y}(t'+1) \text{ for all } t' \leq t) &\geq \mbox{Pr}(Y(t') \in \mathsf{M}^c \text{ for all } t' \leq t) \\
%		&= \int_{\mathsf{M}^c} \mu(\df y_0) \int_{\mathsf{M}^c} \Mtk (y_0, \df y_1) \cdots \int_{\mathsf{M}^c} \Mtk (y_{t-1}, \df y_t) \\
%		&= 1.
%	\end{aligned}
%	\]
%	Letting $t \to \infty$ shows that $(Y(t))_{t=0}^{\infty} = (\hat{Y}(t))_{t=0}^{\infty}$ with probability~1.
%\end{proof}
%
%
%\section{\pcite{robertson2020assessing} Method for Constructing Confidence Intervals} \label{app:robertson}
%
%Assume that $(\hat{\theta}_n)_{n=1}^{\infty}$ is a sequence of estimators of an $\mathbb{R}^d$-valued quantity $\theta$ such that $n^{1/2} (\hat{\theta}_n- \theta) \xrightarrow{d} \mbox{N}_d(0, V)$, where $V$ is some $d \times d$ covariance matrix.
%Assume that we have access to a sequence of consistent estimators of~$V$, denoted by $(\hat{V}_n)_{n=1}^{\infty}$, where $\hat{V}_n \in \mathbb{R}^{d \times d}$.
%
%One can use the following method proposed by \cite{robertson2020assessing} to construct simultaneous confidence intervals for the components of $\theta$.
%For $i = 1,\dots,d$, let $\hat{\theta}_{n,i}$ and $\theta_i$ be the $i$th components of $\hat{\theta}_n$ and $\theta$, respectively.
%Let $\hat{v}_{n,i}$ be the $i$th diagonal element of $\hat{V}_n$.
%For a prescribed nominal joint coverage probability $1- \alpha$, consider using
%\[
%\prod_{i=1}^d \left[ \hat{\theta}_{n,i} - \xi_n(\alpha) \sqrt{\frac{\hat{v}_{n,i}}{n}}, \hat{\theta}_{n,i} + \xi_n(\alpha) \sqrt{\frac{\hat{v}_{n,i}}{n}} \right]
%\]
%as simultaneous confidence intervals for $\theta_1,\dots,\theta_d$.
%(For $\mathsf{A}_1, \dots, \mathsf{A}_m \subset \mathbb{R}$, the notation $\prod_{i=1}^d \mathsf{A}_i$ means the set $\mathsf{A}_1 \times \cdots \times \mathsf{A}_d$.)
%Here, $\xi_n(\alpha)$ is the unique positive solution to the following equation in~$\xi$:
%\begin{equation} \label{eq:z}
%	\mbox{N}_d \left( \prod_{i=1}^d [ -\xi \hat{v}_{n,i}^{1/2}, \xi \hat{v}_{n,i}^{1/2}  ]; 0, \hat{V}_n  \right) = 1 - \alpha,
%\end{equation}
%where $\mbox{N}_d(\cdot; 0, \hat{V}_n)$ denotes the measure given by the $\mbox{N}_d(0, \hat{V}_n)$ distribution.
%Note that the right-hand-side of \eqref{eq:z} is increasing in $\xi$.
%Moreover,  $z_{1-\alpha/2}^* \leq \xi_n(\alpha) \leq z_{1-\alpha/(2m)}^*$, where $z_p^*$ denotes the $p$th quantile of the standard normal distribution.
%For any given $\xi > 0$, the probability on the left in \eqref{eq:z} can be computed using quasi-Monte Carlo methods \citep{genz2009computation,genz2014multivariate}.
%Thus, $\xi_n(\alpha)$ can be estimated using a bisection method between $z_{1-\alpha/2}^*$ and $z_{1-\alpha/(2m)}^*$.
%
%
%It appears that \cite{robertson2020assessing} did not provide a proof for the asymptotic validity of this construction.
%The following result shows that these confidence intervals do give the desired joint coverage probability.
%
%\begin{proposition} \label{pro:confidence}
%	Assume that $V$ is positive definite.
%	Let $1 - \alpha \in (0,1)$ and $\varepsilon > 0$ be arbitrary.
%	For $i = 1,\dots,n$ and $\xi \in [0,\infty)$, let
%	\[
%	J_{n,i}(\xi) = \left[ \hat{\theta}_{n,i} - \xi \sqrt{\frac{\hat{v}_{n,i}}{n}}, \hat{\theta}_{n,i}  + \xi \sqrt{\frac{\hat{v}_{n,i}}{n}} \right].
%	\]
%	Then
%	\[
%	\lim_{n \to \infty} \mbox{Pr} \left( \theta \in \prod_{i=1}^m J_{n,i}(\xi_n(\alpha)) \right) = 1 - \alpha.
%	\]
%	Here, $\mbox{Pr}(\cdot)$ denotes the probability of an event.
%\end{proposition}
%
%\begin{proof}
%	For $i = 1,\dots,d$, let $v_i$ be the $i$th diagonal element of $V$.
%	For $\alpha' \in (0,1)$, let $\xi(\alpha')$ be the unique positive solution of the equation
%	\[
%	\mbox{N}_d \left( \prod_{i=1}^d [ -\xi v_i^{1/2}, \xi v_i^{1/2}  ]; 0, \, V  \right) = 1 - \alpha'.
%	\]
%	Let $\Lambda$ be the diagonal matrix whose $i$th diagonal element is $v_i^{1/2}$ and let $\hat{\Lambda}_n$ be analogously defined with $v_i$ replaced by $\hat{v}_{n,i}$.
%	Since $\hat{V}_n \xrightarrow{p} V$, where the limit is positive definite, the total variation distance between $\mbox{N}_d(\cdot; 0, \Lambda^{-1} V \Lambda^{-1} )$ and $\mbox{N}_d(\cdot; 0, \hat{\Lambda}_n^{-1} \hat{V}_n \hat{\Lambda}_n^{-1} )$ goes to~0 in probability \citep{devroye2018total}.
%	Then, for $\alpha' \in (0,1)$,
%	\begin{equation} \label{eq:alpha'}
%		\begin{aligned}
%			\mbox{N}_d \left( \prod_{i=1}^d [ -\xi(\alpha') \hat{v}_{n,i}^{1/2}, \xi(\alpha') \hat{v}_{n,i}^{1/2}  ]; 0, \hat{V}_n  \right) 
%			=& \mbox{N}_d \left( \prod_{i=1}^d [  -\xi(\alpha'),  \xi(\alpha')  ]; 0, \hat{\Lambda}_n^{-1} \hat{V}_n \hat{\Lambda}_n^{-1}  \right) \\
%			\xrightarrow{p} & \mbox{N}_d \left( \prod_{i=1}^d [  -\xi(\alpha'),  \xi(\alpha')  ]; 0, \Lambda^{-1} V \Lambda^{-1}  \right) \\
%			=& \mbox{N}_d \left( \prod_{i=1}^d [  -\xi(\alpha') \, v_i^{1/2},  \xi(\alpha') \, v_i^{1/2}  ]; 0,  V   \right)
%			=  1 - \alpha'.
%		\end{aligned}
%	\end{equation}
%	Let $\delta > 0$ be less than $\alpha$ and $1 - \alpha$, and let $\mathsf{A}_{n,\delta}$ be the event that $\xi_n(\alpha)$ is in $(\xi(\alpha + \delta), \xi(\alpha - \delta))$.
%	Then~\eqref{eq:alpha'} implies that $\mbox{Pr}(\mathsf{A}_{n,\delta}) \to 1$ as $n \to \infty$, since
%	\[
%	\begin{aligned}
%		\mbox{Pr}(\mathsf{A}_{n,\delta}^c) \leq& \mbox{Pr} \left[ \mbox{N}_d \left( \prod_{i=1}^d [ -\xi(\alpha+\delta) \, \hat{v}_{n,i}^{1/2}, \, \xi(\alpha+\delta) \, \hat{v}_{n,i}^{1/2}  ]; 0, \hat{V}_n  \right) \geq 1 - \alpha \right] + \\
%		& \mbox{Pr} \left[ \mbox{N}_d \left( \prod_{i=1}^d [ -\xi(\alpha-\delta)\, \hat{v}_{n,i}^{1/2}, \, \xi(\alpha-\delta)\, \hat{v}_{n,i}^{1/2}  ]; 0, \hat{V}_n \right) \leq 1 - \alpha \right].
%	\end{aligned}
%	\]
%	Note also that
%	\begin{equation} \label{ine:Prbound}
%		\begin{aligned}
%			&\mbox{Pr} \left( \theta \in \prod_{i=1}^d J_{n,i}(\xi_n(\alpha)) \right) \leq \mbox{Pr}\left( \theta \in \prod_{i=1}^d J_{n,i}(\xi(\alpha - \delta)) \right) + \mbox{Pr}(\mathsf{A}_{n,\delta}^c),
%			\\
%			&\mbox{Pr}\left( \theta \in \prod_{i=1}^d J_{n,i}(\xi(\alpha + \delta)) \right) \leq \mbox{Pr} \left( \theta \in \prod_{i=1}^d J_{n,i}(\xi_n(\alpha)) \right) + \mbox{Pr}(\mathsf{A}_{n,\delta}^c).
%		\end{aligned}
%	\end{equation}
%	By Slutsky's theorem,
%	\[
%	n^{1/2} \hat{\Lambda}_n^{-1} (\hat{\theta}_n - \theta) \xrightarrow{d} \mbox{N}_d(0, \Lambda^{-1} V \Lambda^{-1}).
%	\]
%	By the Portmanteau theorem, for $\alpha' \in (0,1)$,
%	\begin{equation} \label{eq:Prconverge}
%		\begin{aligned}
%			&\lim_{n \to \infty} \mbox{Pr} \left( \theta \in \prod_{i=1}^d J_{n,i}(\xi(\alpha')) \right) \\=& \lim_{n \to \infty} \mbox{Pr} \left( n^{1/2} \hat{\Lambda}_n^{-1} \left( \hat{\theta}_n - \theta \right) \in \prod_{i=1}^d [-\xi(\alpha'), \xi(\alpha')] \right)  \\
%			=& \mbox{N}_d \left( \prod_{i=1}^d [  -\xi(\alpha'),  \xi(\alpha')  ]; 0, \Lambda^{-1} V \Lambda^{-1}  \right) \\
%			=& 1 - \alpha'.
%		\end{aligned}
%	\end{equation}
%	Combining~\eqref{ine:Prbound},~\eqref{eq:Prconverge}, and the fact that $\mbox{Pr}(\mathsf{A}_{n,\delta}) \to 1$ yields
%	\[
%	\lim_{n \to \infty} \mbox{Pr} \left( \theta \in \prod_{i=1}^d J_{n,i}(\xi_n(\alpha)) \right) = 1 - \alpha.
%	\]
%\end{proof}
%
%The above proof requires that $V$ is positive definite.
%This is not always possible.
%As an example, let $\mathsf{K} = \{1,\dots,d\}$, and let $\theta = (\Pi f_1, \dots, \Pi f_{d})$, where, for $k \in \{1,\dots,d\}$, $f_k(k',z') = \ind_{\{k\}}(k'), \; (k',z') \in \X$.
%Let $(X(t))_{t=1}^{\infty}$ be independently drawn from $\Pi$, and let $\hat{\theta}_n = n^{-1} ( \sum_{t=1}^n f_1(X(t)), \dots, \sum_{t=1}^n f_{d}(X(t)) )$.
%Then the covariance matrix of $\hat{\theta}_n$ is singular.
%In this case, one may inject a tiny amount of noise into the estimator $\hat{\theta}_n$, i.e., replace $\hat{\theta}_n$ with $\hat{\theta}_n + \varepsilon G_n$, where $\varepsilon$ is a positive constant, and $G_n$ is the sample average of $n$ iid $\mbox{N}_d(0, I_d)$ random vectors such that $G_n$ and $\hat{\theta}_n$ are independent.
%Inference for $\theta$ is then carried out based on $\hat{\theta}_n + \varepsilon G_n$.
%	
%	
	
	
	\bibliographystyle{Chicago}
	
	\bibliography{qinbib}


\end{document}